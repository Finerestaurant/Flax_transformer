{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Jax/Flax transformer implemetation with wandb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# import module"
      ],
      "metadata": {
        "id": "k3IxQBsRXYIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import json\n",
        "from functools import partial\n",
        "import functools\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "sns.reset_orig()\n",
        "\n",
        "## tqdm for loading bars\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "## To run JAX on TPU in Google Colab, uncomment the two lines below\n",
        "# import jax.tools.colab_tpu\n",
        "# jax.tools.colab_tpu.setup_tpu()\n",
        "\n",
        "## JAX\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "\n",
        "## Flax (NN in JAX)\n",
        "try:\n",
        "    import flax\n",
        "except ModuleNotFoundError: # Install flax if missing\n",
        "    !pip install --quiet flax\n",
        "    import flax\n",
        "    \n",
        "from flax import linen as nn\n",
        "from flax.training import train_state, checkpoints\n",
        "from flax.training import common_utils\n",
        "## Optax (Optimizers in JAX)\n",
        "try:\n",
        "    import optax\n",
        "except ModuleNotFoundError: # Install optax if missing\n",
        "    !pip install --quiet optax\n",
        "    import optax\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "try:\n",
        "    import wandb\n",
        "except ModuleNotFoundError: # Install wandb if missing\n",
        "    !pip install --quiet wandb\n",
        "    import wandb\n",
        "import pandas as pd\n",
        "import re"
      ],
      "metadata": {
        "id": "aki4Re7eWRbs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4785034-2b39-47d7-d35f-533fdb86b421"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 202 kB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 145 kB 40.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 41.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.5 MB 29.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 217 kB 63.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 51 kB 4.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 76 kB 2.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 5.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 42.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 23.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 587 kB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 18.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 39.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 156 kB 10.3 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(\n",
        "    project='Flax-transformer',\n",
        "    entity='seegong'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "tMy_4f-5crPn",
        "outputId": "acb95ec6-aa47-48cf-b18f-9c78e833d60a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220811_001951-3s0ed8uw</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/seegong/Flax-transformer/runs/3s0ed8uw\" target=\"_blank\">atomic-salad-6</a></strong> to <a href=\"https://wandb.ai/seegong/Flax-transformer\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/seegong/Flax-transformer/runs/3s0ed8uw?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f0c01ee3250>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hp_7V4NuWKy8"
      },
      "outputs": [],
      "source": [
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jax.local_devices()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQo6d1ZWWUJD",
        "outputId": "89c6cd6c-3984-4bd2-8638-5f98db9a0d46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
              " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
              " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
              " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
              " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
              " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
              " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
              " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data preprocess"
      ],
      "metadata": {
        "id": "R66UVkDGWWfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://github.com/songys/Chatbot_data/raw/master/ChatbotData.csv'\n",
        "raw_data = pd.read_csv(url)\n",
        "raw_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "fo-oIeSyU9Ky",
        "outputId": "30bf30df-080d-4249-f1c8-043d7d56fe1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                             Q                         A  label\n",
              "0                       12시 땡!                하루가 또 가네요.      0\n",
              "1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n",
              "2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
              "3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
              "4                      PPL 심하네                눈살이 찌푸려지죠.      0\n",
              "...                        ...                       ...    ...\n",
              "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
              "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
              "11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n",
              "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
              "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
              "\n",
              "[11823 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f5dfc400-6a88-46b2-85c8-2d636577a0c1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12시 땡!</td>\n",
              "      <td>하루가 또 가네요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1지망 학교 떨어졌어</td>\n",
              "      <td>위로해 드립니다.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3박4일 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3박4일 정도 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PPL 심하네</td>\n",
              "      <td>눈살이 찌푸려지죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11818</th>\n",
              "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
              "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11819</th>\n",
              "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
              "      <td>훔쳐보는 거 티나나봐요.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11820</th>\n",
              "      <td>흑기사 해주는 짝남.</td>\n",
              "      <td>설렜겠어요.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11821</th>\n",
              "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
              "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11822</th>\n",
              "      <td>힘들어서 결혼할까봐</td>\n",
              "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11823 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f5dfc400-6a88-46b2-85c8-2d636577a0c1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f5dfc400-6a88-46b2-85c8-2d636577a0c1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f5dfc400-6a88-46b2-85c8-2d636577a0c1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_question = raw_data['Q'].values\n",
        "raw_answer = raw_data['A'].values"
      ],
      "metadata": {
        "id": "uifp_jmfVU3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentecne(sentence):\n",
        "    sentence = re.sub(r\"[^a-zA-Z0-9ㄱ-ㅎㅏ-ㅣ가-힣?.!,\\\"']+\", \" \",  sentence) # 알파벳, 문장부호, 한글만 남기고 모두 제거\n",
        "    sentence = sentence.lower().strip()\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "LA_IEOFoVqca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_question = [preprocess_sentecne(sentence) for sentence in raw_question]\n",
        "clean_answer = [preprocess_sentecne(sentence) for sentence in raw_answer]"
      ],
      "metadata": {
        "id": "FhJleTceWAHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.system('apt-get update')\n",
        "os.system('apt-get install g++ openjdk-8-jdk python-dev python3-dev')\n",
        "os.environ['JAVA_HOME'] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.system(\"curl -s -L https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh | bash\")\n",
        "os.system('pip3 install /tmp/mecab-python-0.996')"
      ],
      "metadata": {
        "id": "ao1Od2RERvBV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9945bf4e-9475-48c4-f1e7-2cca8775da1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "lyJ44P8xRwoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()"
      ],
      "metadata": {
        "id": "lz0OzaDvW7Bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mecab_question = [mecab.morphs(sentence) for sentence in clean_question]\n",
        "mecab_answer = [mecab.morphs(sentence) for sentence in clean_answer]"
      ],
      "metadata": {
        "id": "4Ga_bZgxYGHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_corpus = []"
      ],
      "metadata": {
        "id": "qaKQbp_rCHu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_corpus = []\n",
        "for question, answer in zip(mecab_question, mecab_answer):\n",
        "  if len(answer) <= 15 and len(question) <= 15:\n",
        "    raw_corpus.append((question, answer))"
      ],
      "metadata": {
        "id": "TocNVYYOB8Wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "word2vec_path = '/content/drive/MyDrive/Colab Notebooks/ko.bin'\n",
        "word2vec = Word2Vec.load(word2vec_path)"
      ],
      "metadata": {
        "id": "7D2kw-EdkU3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random as rand\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "6mKn3C2NCRgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lexical_sub(sentence, word2vec):\n",
        "    res = []\n",
        "    index_array = list(range(len(sentence)))\n",
        "    rand.shuffle(index_array)\n",
        "    for n, index in enumerate(index_array):\n",
        "      try:      \n",
        "        to = word2vec.wv.most_similar(sentence[index])[0][0]\n",
        "        for tok in sentence:\n",
        "          if tok is sentence[index]: res.append(to)\n",
        "          else: res.append(tok)\n",
        "        break\n",
        "\n",
        "      except:\n",
        "        if n == len(sentence):   \n",
        "          return None\n",
        "        else:\n",
        "          continue\n",
        "\n",
        "    return res"
      ],
      "metadata": {
        "id": "l3KT7odY4Fbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_question_corpus = [(lexical_sub(sentence, word2vec), old_answer) for sentence, old_answer in tqdm(raw_corpus)]"
      ],
      "metadata": {
        "id": "bKxfRRA34SIo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4b51f68-f518-4ce1-985d-3f09f7c8abf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11030/11030 [01:46<00:00, 103.37it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for question, answer in new_question_corpus:\n",
        "  if question is None: continue\n",
        "  total_corpus.append((question, answer))"
      ],
      "metadata": {
        "id": "QEUlMcIb8tOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_answer_corpus = [(old_question, lexical_sub(sentence, word2vec)) for old_question, sentence in tqdm(raw_corpus)]"
      ],
      "metadata": {
        "id": "DTas-_yA6yvr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "761e52a2-c8b0-455b-8310-c589361ddc62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11030/11030 [01:11<00:00, 154.02it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for question, answer in new_answer_corpus:\n",
        "  if answer is None: continue\n",
        "  total_corpus.append((question, answer))"
      ],
      "metadata": {
        "id": "4mIpKf1R9fv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for question, answer in raw_corpus:\n",
        "  total_corpus.append((question, answer))"
      ],
      "metadata": {
        "id": "_VeACueiNBPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_dict = {}\n",
        "for question, answer in total_corpus:\n",
        "  for word in question:\n",
        "    try:\n",
        "      word_dict[word] += 1\n",
        "    except:\n",
        "      word_dict[word] = 0\n",
        "  for word in answer:\n",
        "    try:\n",
        "      word_dict[word] += 1\n",
        "    except:\n",
        "      word_dict[word] = 0\n",
        "\n",
        "word_dict['<PAD>'] = max(word_dict.values())+1"
      ],
      "metadata": {
        "id": "0fwF0zarh6mU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_word_index = {k:v for v,k in enumerate(dict(sorted(word_dict.items(), key=lambda x: x[1], reverse=True)).keys())}\n",
        "src_index_word = {k:v for v,k in src_word_index.items()}"
      ],
      "metadata": {
        "id": "IYhuh6EoilGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 워드 딕셔너리는 아주 잘 생성되었다."
      ],
      "metadata": {
        "id": "4rZNjwh3-fOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def change_to_tensor(sentence, word_dict):\n",
        "  sentence = [word_dict[word] for word in sentence]\n",
        "  if len(sentence) < 15:\n",
        "    for i in range(15 - len(sentence)):\n",
        "      sentence.append(0)\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "XLpmBxG_-xIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_tensor = [(change_to_tensor(question, src_word_index), change_to_tensor(answer, src_word_index))  for question, answer in total_corpus]"
      ],
      "metadata": {
        "id": "BMTwlj7a-jZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_tensor = jnp.array([array[0] for array in total_tensor])\n",
        "question_tensor"
      ],
      "metadata": {
        "id": "mDMye0fyNcZz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e5f5b42-a410-4751-aaeb-6b0201dbab1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[2422,  169, 3454, ...,    0,    0,    0],\n",
              "             [ 278, 4538, 3455, ...,    0,    0,    0],\n",
              "             [ 294, 2423,  603, ...,    0,    0,    0],\n",
              "             ...,\n",
              "             [6808,   12,   55, ...,    0,    0,    0],\n",
              "             [ 199,  121,   11, ...,    0,    0,    0],\n",
              "             [  67,  113,  159, ...,    0,    0,    0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_tensor = jnp.array([array[1] for array in total_tensor])\n",
        "answer_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOCw780LR7-6",
        "outputId": "2205b35d-a928-4733-d0b0-c428942e8435"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[ 282,    7,  138, ...,    0,    0,    0],\n",
              "             [ 527,   12, 1410, ...,    0,    0,    0],\n",
              "             [ 251,   14,  686, ...,    0,    0,    0],\n",
              "             ...,\n",
              "             [2678,   23,   27, ...,    0,    0,    0],\n",
              "             [  44, 1953,   42, ...,    0,    0,    0],\n",
              "             [4417,  159,   14, ...,    0,    0,    0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model architecture"
      ],
      "metadata": {
        "id": "y3CTyJAnXrTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable, Any, Optional\n",
        "\n",
        "from flax import linen as nn\n",
        "from flax import struct\n",
        "from jax import lax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "CZ1wTtGR69D3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TransformerConfig"
      ],
      "metadata": {
        "id": "Gt7IW30jAy1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@struct.dataclass\n",
        "class TransformerConfig:\n",
        "  \"\"\"Global hyperparameters used to minimize obnoxious kwarg plumbing.\"\"\"\n",
        "  vocab_size: int = len(src_word_index)\n",
        "  output_vocab_size: int = len(src_word_index)\n",
        "  share_embeddings: bool = True\n",
        "  logits_via_embedding: bool = False\n",
        "  dtype: Any = jnp.float32\n",
        "  emb_dim: int = 512\n",
        "  num_heads: int = 8\n",
        "  num_layers: int = 6\n",
        "  qkv_dim: int = 512\n",
        "  mlp_dim: int = 2048\n",
        "  max_len: int = 2048\n",
        "  dropout_rate: float = 0.1\n",
        "  attention_dropout_rate: float = 0.1\n",
        "  deterministic: bool = False\n",
        "  decode: bool = False\n",
        "  learning_rate: float = 0.01\n",
        "  warmup_steps: int = 10\n",
        "  kernel_init: Callable = nn.initializers.xavier_uniform()\n",
        "  bias_init: Callable = nn.initializers.normal(stddev=1e-6)\n",
        "  posemb_init: Optional[Callable] = None\n",
        "  label_smoothing: float = 0.1"
      ],
      "metadata": {
        "id": "HWUvuwPL64Zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shift_right(x, axis=1):\n",
        "  \"\"\"Shift the input to the right by padding on axis 1.\"\"\"\n",
        "  pad_widths = [(0, 0)] * len(x.shape)\n",
        "  pad_widths[axis] = (1, 0)\n",
        "  padded = jnp.pad(\n",
        "      x, pad_widths, mode='constant', constant_values=x.dtype.type(0))\n",
        "  return padded[:, :-1]"
      ],
      "metadata": {
        "id": "JR0Bk8wb7kBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sinusoidal_init(max_len=2048,\n",
        "                    min_scale=1.0,\n",
        "                    max_scale=10000.0):\n",
        "  \"\"\"1D Sinusoidal Position Embedding Initializer.\n",
        "\n",
        "  Args:\n",
        "      max_len: maximum possible length for the input.\n",
        "      min_scale: float: minimum frequency-scale in sine grating.\n",
        "      max_scale: float: maximum frequency-scale in sine grating.\n",
        "\n",
        "  Returns:\n",
        "      output: init function returning `(1, max_len, d_feature)`\n",
        "  \"\"\"\n",
        "\n",
        "  def init(key, shape, dtype=np.float32):\n",
        "    \"\"\"Sinusoidal init.\"\"\"\n",
        "    del key, dtype\n",
        "    d_feature = shape[-1]\n",
        "    pe = np.zeros((max_len, d_feature), dtype=np.float32)\n",
        "    position = np.arange(0, max_len)[:, np.newaxis]\n",
        "    scale_factor = -np.log(max_scale / min_scale) / (d_feature // 2 - 1)\n",
        "    div_term = min_scale * np.exp(np.arange(0, d_feature // 2) * scale_factor)\n",
        "    pe[:, :d_feature // 2] = np.sin(position * div_term)\n",
        "    pe[:, d_feature // 2: 2 * (d_feature // 2)] = np.cos(position * div_term)\n",
        "    pe = pe[np.newaxis, :, :]  # [1, max_len, d_feature]\n",
        "    return jnp.array(pe)\n",
        "\n",
        "  return init"
      ],
      "metadata": {
        "id": "E8itm4e57w5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AddPositionEmbs(nn.Module):\n",
        "  \"\"\"Adds (optionally learned) positional embeddings to the inputs.\n",
        "\n",
        "  Attributes:\n",
        "    config: TransformerConfig dataclass containing hyperparameters.\n",
        "    decode: whether to run in single-position autoregressive mode.\n",
        "  \"\"\"\n",
        "  config: TransformerConfig\n",
        "  decode: bool = False\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self,\n",
        "               inputs,\n",
        "               inputs_positions=None):\n",
        "    \"\"\"Applies AddPositionEmbs module.\n",
        "\n",
        "    By default this layer uses a fixed sinusoidal embedding table. If a\n",
        "    learned position embedding is desired, pass an initializer to\n",
        "    posemb_init in the configuration.\n",
        "\n",
        "    Args:\n",
        "      inputs: input data.\n",
        "      inputs_positions: input position indices for packed sequences.\n",
        "\n",
        "    Returns:\n",
        "      output: `(bs, timesteps, in_dim)`\n",
        "    \"\"\"\n",
        "    config = self.config\n",
        "    # inputs.shape is (batch_size, seq_len, emb_dim)\n",
        "    assert inputs.ndim == 3, ('Number of dimensions should be 3,'\n",
        "                              ' but it is: %d' % inputs.ndim)\n",
        "    length = inputs.shape[1]\n",
        "    pos_emb_shape = (1, config.max_len, inputs.shape[-1])\n",
        "    if config.posemb_init is None:\n",
        "      # Use a fixed (non-learned) sinusoidal position embedding.\n",
        "      pos_embedding = sinusoidal_init(max_len=config.max_len)(None,\n",
        "                                                              pos_emb_shape,\n",
        "                                                              None)\n",
        "    else:\n",
        "      pos_embedding = self.param('pos_embedding', config.posemb_init,\n",
        "                                 pos_emb_shape)\n",
        "    pe = pos_embedding[:, :length, :]\n",
        "\n",
        "    # We use a cache position index for tracking decoding position.\n",
        "    if self.decode:\n",
        "      is_initialized = self.has_variable('cache', 'cache_index')\n",
        "      cache_index = self.variable('cache', 'cache_index',\n",
        "                                  lambda: jnp.array(0, dtype=jnp.uint32))\n",
        "      if is_initialized:\n",
        "        i = cache_index.value\n",
        "        cache_index.value = i + 1\n",
        "        _, _, df = pos_embedding.shape\n",
        "        pe = lax.dynamic_slice(pos_embedding,\n",
        "                               jnp.array((0, i, 0)),\n",
        "                               (1, 1, df))\n",
        "    if inputs_positions is None:\n",
        "      # normal unpacked case:\n",
        "      return inputs + pe\n",
        "    else:\n",
        "      # for packed data we need to use known position indices:\n",
        "      return inputs + jnp.take(pe[0], inputs_positions, axis=0)"
      ],
      "metadata": {
        "id": "gIabUj-o-AhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MlpBlock(nn.Module):\n",
        "  \"\"\"Transformer MLP / feed-forward block.\n",
        "\n",
        "  Attributes:\n",
        "    config: TransformerConfig dataclass containing hyperparameters.\n",
        "    out_dim: optionally specify out dimension.\n",
        "  \"\"\"\n",
        "  config: TransformerConfig\n",
        "  out_dim: Optional[int] = None\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs):\n",
        "    \"\"\"Applies Transformer MlpBlock module.\"\"\"\n",
        "    config = self.config\n",
        "    actual_out_dim = (inputs.shape[-1] if self.out_dim is None\n",
        "                      else self.out_dim)\n",
        "    x = nn.Dense(\n",
        "        config.mlp_dim,\n",
        "        dtype=config.dtype,\n",
        "        kernel_init=config.kernel_init,\n",
        "        bias_init=config.bias_init)(\n",
        "            inputs)\n",
        "    x = nn.relu(x)\n",
        "    x = nn.Dropout(rate=config.dropout_rate)(\n",
        "        x, deterministic=config.deterministic)\n",
        "    output = nn.Dense(\n",
        "        actual_out_dim,\n",
        "        dtype=config.dtype,\n",
        "        kernel_init=config.kernel_init,\n",
        "        bias_init=config.bias_init)(\n",
        "            x)\n",
        "    output = nn.Dropout(rate=config.dropout_rate)(\n",
        "        output, deterministic=config.deterministic)\n",
        "    return output"
      ],
      "metadata": {
        "id": "tezcQvu4K5md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder1DBlock(nn.Module):\n",
        "  \"\"\"Transformer encoder layer.\n",
        "\n",
        "  Attributes:\n",
        "    config: TransformerConfig dataclass containing hyperparameters.\n",
        "  \"\"\"\n",
        "  config: TransformerConfig\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self,\n",
        "               inputs,\n",
        "               encoder_mask=None):\n",
        "    \"\"\"Applies Encoder1DBlock module.\n",
        "\n",
        "    Args:\n",
        "      inputs: input data.\n",
        "      encoder_mask: encoder self-attention mask.\n",
        "\n",
        "    Returns:\n",
        "      output after transformer encoder block.\n",
        "    \"\"\"\n",
        "    config = self.config\n",
        "\n",
        "    # Attention block.\n",
        "    assert inputs.ndim == 3\n",
        "    x = nn.LayerNorm(dtype=config.dtype)(inputs)\n",
        "    x = nn.SelfAttention(\n",
        "        num_heads=config.num_heads,\n",
        "        dtype=config.dtype,\n",
        "        qkv_features=config.qkv_dim,\n",
        "        kernel_init=config.kernel_init,\n",
        "        bias_init=config.bias_init,\n",
        "        use_bias=False,\n",
        "        broadcast_dropout=False,\n",
        "        dropout_rate=config.attention_dropout_rate,\n",
        "        deterministic=config.deterministic)(x, encoder_mask)\n",
        "\n",
        "    x = nn.Dropout(rate=config.dropout_rate)(\n",
        "        x, deterministic=config.deterministic)\n",
        "    x = x + inputs\n",
        "\n",
        "    # MLP block.\n",
        "    y = nn.LayerNorm(dtype=config.dtype)(x)\n",
        "    y = MlpBlock(config=config)(y)\n",
        "\n",
        "    return x + y"
      ],
      "metadata": {
        "id": "tralllF1GWEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoder1DBlock(nn.Module):\n",
        "  \"\"\"Transformer encoder-decoder layer.\n",
        "\n",
        "  Attributes:\n",
        "    config: TransformerConfig dataclass containing hyperparameters.\n",
        "  \"\"\"\n",
        "  config: TransformerConfig\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self,\n",
        "               targets,\n",
        "               encoded,\n",
        "               decoder_mask=None,\n",
        "               encoder_decoder_mask=None):\n",
        "    \"\"\"Applies EncoderDecoder1DBlock module.\n",
        "\n",
        "    Args:\n",
        "      targets: input data for decoder\n",
        "      encoded: input data from encoder\n",
        "      decoder_mask: decoder self-attention mask.\n",
        "      encoder_decoder_mask: encoder-decoder attention mask.\n",
        "\n",
        "    Returns:\n",
        "      output after transformer encoder-decoder block.\n",
        "    \"\"\"\n",
        "    config = self.config\n",
        "\n",
        "    # Decoder block.\n",
        "    assert targets.ndim == 3\n",
        "    x = nn.LayerNorm(dtype=config.dtype)(targets)\n",
        "    x = nn.SelfAttention(\n",
        "        num_heads=config.num_heads,\n",
        "        dtype=config.dtype,\n",
        "        qkv_features=config.qkv_dim,\n",
        "        kernel_init=config.kernel_init,\n",
        "        bias_init=config.bias_init,\n",
        "        use_bias=False,\n",
        "        broadcast_dropout=False,\n",
        "        dropout_rate=config.attention_dropout_rate,\n",
        "        deterministic=config.deterministic,\n",
        "        decode=config.decode)(x, decoder_mask)\n",
        "    x = nn.Dropout(rate=config.dropout_rate)(\n",
        "        x, deterministic=config.deterministic)\n",
        "    x = x + targets\n",
        "\n",
        "    # Encoder-Decoder block.\n",
        "    y = nn.LayerNorm(dtype=config.dtype)(x)\n",
        "    y = nn.MultiHeadDotProductAttention(\n",
        "        num_heads=config.num_heads,\n",
        "        dtype=config.dtype,\n",
        "        qkv_features=config.qkv_dim,\n",
        "        kernel_init=config.kernel_init,\n",
        "        bias_init=config.bias_init,\n",
        "        use_bias=False,\n",
        "        broadcast_dropout=False,\n",
        "        dropout_rate=config.attention_dropout_rate,\n",
        "        deterministic=config.deterministic)(y, encoded, encoder_decoder_mask)\n",
        "\n",
        "    y = nn.Dropout(rate=config.dropout_rate)(\n",
        "        y, deterministic=config.deterministic)\n",
        "    y = y + x\n",
        "\n",
        "    # MLP block.\n",
        "    z = nn.LayerNorm(dtype=config.dtype)(y)\n",
        "    z = MlpBlock(config=config)(z)\n",
        "\n",
        "    return y + z"
      ],
      "metadata": {
        "id": "EPaIzQ2DGYlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  \"\"\"Transformer Model Encoder for sequence to sequence translation.\n",
        "\n",
        "  Attributes:\n",
        "    config: TransformerConfig dataclass containing hyperparameters.\n",
        "    shared_embedding: a shared embedding layer to use.\n",
        "  \"\"\"\n",
        "  config: TransformerConfig\n",
        "  shared_embedding: Any = None\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self,\n",
        "               inputs,\n",
        "               inputs_positions=None,\n",
        "               encoder_mask=None):\n",
        "    \"\"\"Applies Transformer model on the inputs.\n",
        "\n",
        "    Args:\n",
        "      inputs: input data\n",
        "      inputs_positions: input subsequence positions for packed examples.\n",
        "      encoder_mask: decoder self-attention mask.\n",
        "\n",
        "    Returns:\n",
        "      output of a transformer encoder.\n",
        "    \"\"\"\n",
        "    config = self.config\n",
        "    assert inputs.ndim == 2  # (batch, len)\n",
        "\n",
        "    # Input Embedding\n",
        "    if self.shared_embedding is None:\n",
        "      input_embed = nn.Embed(\n",
        "          num_embeddings=config.vocab_size,\n",
        "          features=config.emb_dim,\n",
        "          embedding_init=nn.initializers.normal(stddev=1.0))\n",
        "    else:\n",
        "      input_embed = self.shared_embedding\n",
        "    x = inputs.astype('int32')\n",
        "    x = input_embed(x)\n",
        "    x = AddPositionEmbs(\n",
        "        config=config, decode=False, name='posembed_input')(\n",
        "            x, inputs_positions=inputs_positions)\n",
        "    x = nn.Dropout(rate=config.dropout_rate)(\n",
        "        x, deterministic=config.deterministic)\n",
        "\n",
        "    x = x.astype(config.dtype)\n",
        "\n",
        "    # Input Encoder\n",
        "    for lyr in range(config.num_layers):\n",
        "      x = Encoder1DBlock(\n",
        "          config=config, name=f'encoderblock_{lyr}')(x, encoder_mask)\n",
        "\n",
        "    encoded = nn.LayerNorm(dtype=config.dtype, name='encoder_norm')(x)\n",
        "\n",
        "    return encoded\n"
      ],
      "metadata": {
        "id": "niSySY9zQUGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  \"\"\"Transformer Model Decoder for sequence to sequence translation.\n",
        "\n",
        "  Attributes:\n",
        "    config: TransformerConfig dataclass containing hyperparameters.\n",
        "    shared_embedding: a shared embedding layer to use.\n",
        "  \"\"\"\n",
        "  config: TransformerConfig\n",
        "  shared_embedding: Any = None\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self,\n",
        "               encoded,\n",
        "               targets,\n",
        "               targets_positions=None,\n",
        "               decoder_mask=None,\n",
        "               encoder_decoder_mask=None):\n",
        "    \"\"\"Applies Transformer model on the inputs.\n",
        "\n",
        "    Args:\n",
        "      encoded: encoded input data from encoder.\n",
        "      targets: target inputs.\n",
        "      targets_positions: input subsequence positions for packed examples.\n",
        "      decoder_mask: decoder self-attention mask.\n",
        "      encoder_decoder_mask: encoder-decoder attention mask.\n",
        "\n",
        "    Returns:\n",
        "      output of a transformer decoder.\n",
        "    \"\"\"\n",
        "    config = self.config\n",
        "\n",
        "    assert encoded.ndim == 3  # (batch, len, depth)\n",
        "    assert targets.ndim == 2  # (batch, len)\n",
        "\n",
        "    # Target Embedding\n",
        "    if self.shared_embedding is None:\n",
        "      output_embed = nn.Embed(\n",
        "          num_embeddings=config.output_vocab_size,\n",
        "          features=config.emb_dim,\n",
        "          embedding_init=nn.initializers.normal(stddev=1.0))\n",
        "    else:\n",
        "      output_embed = self.shared_embedding\n",
        "\n",
        "    y = targets.astype('int32')\n",
        "    if not config.decode:\n",
        "      y = shift_right(y)\n",
        "    y = output_embed(y)\n",
        "    y = AddPositionEmbs(\n",
        "        config=config, decode=config.decode, name='posembed_output')(\n",
        "            y, inputs_positions=targets_positions)\n",
        "    y = nn.Dropout(rate=config.dropout_rate)(\n",
        "        y, deterministic=config.deterministic)\n",
        "\n",
        "    y = y.astype(config.dtype)\n",
        "\n",
        "    # Target-Input Decoder\n",
        "    for lyr in range(config.num_layers):\n",
        "      y = EncoderDecoder1DBlock(\n",
        "          config=config, name=f'encoderdecoderblock_{lyr}')(\n",
        "              y,\n",
        "              encoded,\n",
        "              decoder_mask=decoder_mask,\n",
        "              encoder_decoder_mask=encoder_decoder_mask)\n",
        "    y = nn.LayerNorm(dtype=config.dtype, name='encoderdecoder_norm')(y)\n",
        "\n",
        "    # Decoded Logits\n",
        "    if config.logits_via_embedding:\n",
        "      # Use the transpose of embedding matrix for logit transform.\n",
        "      logits = output_embed.attend(y.astype(jnp.float32))\n",
        "      # Correctly normalize pre-softmax logits for this shared case.\n",
        "      logits = logits / jnp.sqrt(y.shape[-1])\n",
        "    else:\n",
        "      logits = nn.Dense(\n",
        "          config.output_vocab_size,\n",
        "          dtype=config.dtype,\n",
        "          kernel_init=config.kernel_init,\n",
        "          bias_init=config.bias_init,\n",
        "          name='logitdense')(\n",
        "              y)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "ygjyQXw0QQ-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  \"\"\"Transformer Model for sequence to sequence translation.\n",
        "\n",
        "  Attributes:\n",
        "    config: TransformerConfig dataclass containing hyperparameters.\n",
        "  \"\"\"\n",
        "  config: TransformerConfig\n",
        "\n",
        "  def setup(self):\n",
        "    config = self.config\n",
        "\n",
        "    if config.share_embeddings:\n",
        "      if config.output_vocab_size is not None:\n",
        "        assert config.output_vocab_size == config.vocab_size, (\n",
        "            \"can't share embedding with different vocab sizes.\")\n",
        "      self.shared_embedding = nn.Embed(\n",
        "          num_embeddings=config.vocab_size,\n",
        "          features=config.emb_dim,\n",
        "          embedding_init=nn.initializers.normal(stddev=1.0))\n",
        "    else:\n",
        "      self.shared_embedding = None\n",
        "\n",
        "    self.encoder = Encoder(\n",
        "        config=config, shared_embedding=self.shared_embedding)\n",
        "    self.decoder = Decoder(\n",
        "        config=config, shared_embedding=self.shared_embedding)\n",
        "\n",
        "  def encode(self,\n",
        "             inputs,\n",
        "             inputs_positions=None,\n",
        "             inputs_segmentation=None):\n",
        "    \"\"\"Applies Transformer encoder-branch on the inputs.\n",
        "\n",
        "    Args:\n",
        "      inputs: input data.\n",
        "      inputs_positions: input subsequence positions for packed examples.\n",
        "      inputs_segmentation: input segmentation info for packed examples.\n",
        "\n",
        "    Returns:\n",
        "      encoded feature array from the transformer encoder.\n",
        "    \"\"\"\n",
        "    config = self.config\n",
        "    # Make padding attention mask.\n",
        "    encoder_mask = nn.make_attention_mask(\n",
        "        inputs > 0, inputs > 0, dtype=config.dtype)\n",
        "    # Add segmentation block-diagonal attention mask if using segmented data.\n",
        "    if inputs_segmentation is not None:\n",
        "      encoder_mask = nn.combine_masks(\n",
        "          encoder_mask,\n",
        "          nn.make_attention_mask(\n",
        "              inputs_segmentation,\n",
        "              inputs_segmentation,\n",
        "              jnp.equal,\n",
        "              dtype=config.dtype))\n",
        "    return self.encoder(\n",
        "        inputs,\n",
        "        inputs_positions=inputs_positions,\n",
        "        encoder_mask=encoder_mask)\n",
        "\n",
        "  def decode(self,\n",
        "             encoded,\n",
        "             inputs,  # only needed for masks\n",
        "             targets,\n",
        "             targets_positions=None,\n",
        "             inputs_segmentation=None,\n",
        "             targets_segmentation=None):\n",
        "    \"\"\"Applies Transformer decoder-branch on encoded-input and target.\n",
        "\n",
        "    Args:\n",
        "      encoded: encoded input data from encoder.\n",
        "      inputs: input data (only needed for masking).\n",
        "      targets: target data.\n",
        "      targets_positions: target subsequence positions for packed examples.\n",
        "      inputs_segmentation: input segmentation info for packed examples.\n",
        "      targets_segmentation: target segmentation info for packed examples.\n",
        "\n",
        "    Returns:\n",
        "      logits array from transformer decoder.\n",
        "    \"\"\"\n",
        "    config = self.config\n",
        "\n",
        "    # Make padding attention masks.\n",
        "    if config.decode:\n",
        "      # for fast autoregressive decoding only a special encoder-decoder mask is used\n",
        "      decoder_mask = None\n",
        "      encoder_decoder_mask = nn.make_attention_mask(\n",
        "          jnp.ones_like(targets) > 0, inputs > 0, dtype=config.dtype)\n",
        "    else:\n",
        "      decoder_mask = nn.combine_masks(\n",
        "          nn.make_attention_mask(targets > 0, targets > 0, dtype=config.dtype),\n",
        "          nn.make_causal_mask(targets, dtype=config.dtype))\n",
        "      encoder_decoder_mask = nn.make_attention_mask(\n",
        "          targets > 0, inputs > 0, dtype=config.dtype)\n",
        "\n",
        "    # Add segmentation block-diagonal attention masks if using segmented data.\n",
        "    if inputs_segmentation is not None:\n",
        "      decoder_mask = nn.combine_masks(\n",
        "          decoder_mask,\n",
        "          nn.make_attention_mask(\n",
        "              targets_segmentation,\n",
        "              targets_segmentation,\n",
        "              jnp.equal,\n",
        "              dtype=config.dtype))\n",
        "      encoder_decoder_mask = nn.combine_masks(\n",
        "          encoder_decoder_mask,\n",
        "          nn.make_attention_mask(\n",
        "              targets_segmentation,\n",
        "              inputs_segmentation,\n",
        "              jnp.equal,\n",
        "              dtype=config.dtype))\n",
        "    logits = self.decoder(\n",
        "        encoded,\n",
        "        targets,\n",
        "        targets_positions=targets_positions,\n",
        "        decoder_mask=decoder_mask,\n",
        "        encoder_decoder_mask=encoder_decoder_mask)\n",
        "    return logits.astype(self.config.dtype)\n",
        "\n",
        "  def __call__(self,\n",
        "               inputs,\n",
        "               targets,\n",
        "               inputs_positions=None,\n",
        "               targets_positions=None,\n",
        "               inputs_segmentation=None,\n",
        "               targets_segmentation=None):\n",
        "    \"\"\"Applies Transformer model on the inputs.\n",
        "\n",
        "    Args:\n",
        "      inputs: input data.\n",
        "      targets: target data.\n",
        "      inputs_positions: input subsequence positions for packed examples.\n",
        "      targets_positions: target subsequence positions for packed examples.\n",
        "      inputs_segmentation: input segmentation info for packed examples.\n",
        "      targets_segmentation: target segmentation info for packed examples.\n",
        "\n",
        "    Returns:\n",
        "      logits array from full transformer.\n",
        "    \"\"\"\n",
        "    encoded = self.encode(inputs,\n",
        "                          inputs_positions=inputs_positions,\n",
        "                          inputs_segmentation=inputs_segmentation)\n",
        "\n",
        "    return self.decode(encoded,\n",
        "                       inputs,  # only used for masks\n",
        "                       targets,\n",
        "                       targets_positions=targets_positions,\n",
        "                       inputs_segmentation=inputs_segmentation,\n",
        "                       targets_segmentation=targets_segmentation)"
      ],
      "metadata": {
        "id": "5B1nmQpI6lkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# run "
      ],
      "metadata": {
        "id": "htgI1fIvjXYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = TransformerConfig()"
      ],
      "metadata": {
        "id": "Q7rhllMgjAyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = jnp.ones(shape=(128, 15),dtype='bfloat16')\n",
        "y = jnp.ones(shape=(128, 15),dtype='bfloat16')\n",
        "rng, init_rng, dropout_rng = jax.random.split(random.PRNGKey(42), 3)\n",
        "model = Transformer(config)\n",
        "initial_variables = model.init({'params': init_rng, 'dropout': dropout_rng},\n",
        "                                  x,\n",
        "                                  y,\n",
        "                                  )\n",
        "jax.tree_map(lambda x: x.shape, initial_variables)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvA_aORki5T8",
        "outputId": "89835911-78ec-4678-a171-34d38a182e0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FrozenDict({\n",
              "    params: {\n",
              "        decoder: {\n",
              "            encoderdecoder_norm: {\n",
              "                bias: (512,),\n",
              "                scale: (512,),\n",
              "            },\n",
              "            encoderdecoderblock_0: {\n",
              "                LayerNorm_0: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_1: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_2: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                MlpBlock_0: {\n",
              "                    Dense_0: {\n",
              "                        bias: (2048,),\n",
              "                        kernel: (512, 2048),\n",
              "                    },\n",
              "                    Dense_1: {\n",
              "                        bias: (512,),\n",
              "                        kernel: (2048, 512),\n",
              "                    },\n",
              "                },\n",
              "                MultiHeadDotProductAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "                SelfAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "            },\n",
              "            encoderdecoderblock_1: {\n",
              "                LayerNorm_0: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_1: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_2: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                MlpBlock_0: {\n",
              "                    Dense_0: {\n",
              "                        bias: (2048,),\n",
              "                        kernel: (512, 2048),\n",
              "                    },\n",
              "                    Dense_1: {\n",
              "                        bias: (512,),\n",
              "                        kernel: (2048, 512),\n",
              "                    },\n",
              "                },\n",
              "                MultiHeadDotProductAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "                SelfAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "            },\n",
              "            encoderdecoderblock_2: {\n",
              "                LayerNorm_0: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_1: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_2: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                MlpBlock_0: {\n",
              "                    Dense_0: {\n",
              "                        bias: (2048,),\n",
              "                        kernel: (512, 2048),\n",
              "                    },\n",
              "                    Dense_1: {\n",
              "                        bias: (512,),\n",
              "                        kernel: (2048, 512),\n",
              "                    },\n",
              "                },\n",
              "                MultiHeadDotProductAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "                SelfAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "            },\n",
              "            encoderdecoderblock_3: {\n",
              "                LayerNorm_0: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_1: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_2: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                MlpBlock_0: {\n",
              "                    Dense_0: {\n",
              "                        bias: (2048,),\n",
              "                        kernel: (512, 2048),\n",
              "                    },\n",
              "                    Dense_1: {\n",
              "                        bias: (512,),\n",
              "                        kernel: (2048, 512),\n",
              "                    },\n",
              "                },\n",
              "                MultiHeadDotProductAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "                SelfAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "            },\n",
              "            encoderdecoderblock_4: {\n",
              "                LayerNorm_0: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_1: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_2: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                MlpBlock_0: {\n",
              "                    Dense_0: {\n",
              "                        bias: (2048,),\n",
              "                        kernel: (512, 2048),\n",
              "                    },\n",
              "                    Dense_1: {\n",
              "                        bias: (512,),\n",
              "                        kernel: (2048, 512),\n",
              "                    },\n",
              "                },\n",
              "                MultiHeadDotProductAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "                SelfAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "            },\n",
              "            encoderdecoderblock_5: {\n",
              "                LayerNorm_0: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_1: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_2: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                MlpBlock_0: {\n",
              "                    Dense_0: {\n",
              "                        bias: (2048,),\n",
              "                        kernel: (512, 2048),\n",
              "                    },\n",
              "                    Dense_1: {\n",
              "                        bias: (512,),\n",
              "                        kernel: (2048, 512),\n",
              "                    },\n",
              "                },\n",
              "                MultiHeadDotProductAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "                SelfAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "            },\n",
              "            logitdense: {\n",
              "                bias: (7690,),\n",
              "                kernel: (512, 7690),\n",
              "            },\n",
              "        },\n",
              "        encoder: {\n",
              "            encoder_norm: {\n",
              "                bias: (512,),\n",
              "                scale: (512,),\n",
              "            },\n",
              "            encoderblock_0: {\n",
              "                LayerNorm_0: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_1: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                MlpBlock_0: {\n",
              "                    Dense_0: {\n",
              "                        bias: (2048,),\n",
              "                        kernel: (512, 2048),\n",
              "                    },\n",
              "                    Dense_1: {\n",
              "                        bias: (512,),\n",
              "                        kernel: (2048, 512),\n",
              "                    },\n",
              "                },\n",
              "                SelfAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "            },\n",
              "            encoderblock_1: {\n",
              "                LayerNorm_0: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_1: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                MlpBlock_0: {\n",
              "                    Dense_0: {\n",
              "                        bias: (2048,),\n",
              "                        kernel: (512, 2048),\n",
              "                    },\n",
              "                    Dense_1: {\n",
              "                        bias: (512,),\n",
              "                        kernel: (2048, 512),\n",
              "                    },\n",
              "                },\n",
              "                SelfAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "            },\n",
              "            encoderblock_2: {\n",
              "                LayerNorm_0: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_1: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                MlpBlock_0: {\n",
              "                    Dense_0: {\n",
              "                        bias: (2048,),\n",
              "                        kernel: (512, 2048),\n",
              "                    },\n",
              "                    Dense_1: {\n",
              "                        bias: (512,),\n",
              "                        kernel: (2048, 512),\n",
              "                    },\n",
              "                },\n",
              "                SelfAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "            },\n",
              "            encoderblock_3: {\n",
              "                LayerNorm_0: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_1: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                MlpBlock_0: {\n",
              "                    Dense_0: {\n",
              "                        bias: (2048,),\n",
              "                        kernel: (512, 2048),\n",
              "                    },\n",
              "                    Dense_1: {\n",
              "                        bias: (512,),\n",
              "                        kernel: (2048, 512),\n",
              "                    },\n",
              "                },\n",
              "                SelfAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "            },\n",
              "            encoderblock_4: {\n",
              "                LayerNorm_0: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_1: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                MlpBlock_0: {\n",
              "                    Dense_0: {\n",
              "                        bias: (2048,),\n",
              "                        kernel: (512, 2048),\n",
              "                    },\n",
              "                    Dense_1: {\n",
              "                        bias: (512,),\n",
              "                        kernel: (2048, 512),\n",
              "                    },\n",
              "                },\n",
              "                SelfAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "            },\n",
              "            encoderblock_5: {\n",
              "                LayerNorm_0: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_1: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                MlpBlock_0: {\n",
              "                    Dense_0: {\n",
              "                        bias: (2048,),\n",
              "                        kernel: (512, 2048),\n",
              "                    },\n",
              "                    Dense_1: {\n",
              "                        bias: (512,),\n",
              "                        kernel: (2048, 512),\n",
              "                    },\n",
              "                },\n",
              "                SelfAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "            },\n",
              "        },\n",
              "        shared_embedding: {\n",
              "            embedding: (7690, 512),\n",
              "        },\n",
              "    },\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jax.tree_map(lambda y: y.shape, initial_variables)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkY3hjIjkSan",
        "outputId": "86e0f41b-30c7-4248-ce0c-122e00569731"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FrozenDict({\n",
              "    params: {\n",
              "        decoder: {\n",
              "            encoderdecoder_norm: {\n",
              "                bias: (512,),\n",
              "                scale: (512,),\n",
              "            },\n",
              "            encoderdecoderblock_0: {\n",
              "                LayerNorm_0: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_1: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_2: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                MlpBlock_0: {\n",
              "                    Dense_0: {\n",
              "                        bias: (2048,),\n",
              "                        kernel: (512, 2048),\n",
              "                    },\n",
              "                    Dense_1: {\n",
              "                        bias: (512,),\n",
              "                        kernel: (2048, 512),\n",
              "                    },\n",
              "                },\n",
              "                MultiHeadDotProductAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "                SelfAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "            },\n",
              "            encoderdecoderblock_1: {\n",
              "                LayerNorm_0: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_1: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_2: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                MlpBlock_0: {\n",
              "                    Dense_0: {\n",
              "                        bias: (2048,),\n",
              "                        kernel: (512, 2048),\n",
              "                    },\n",
              "                    Dense_1: {\n",
              "                        bias: (512,),\n",
              "                        kernel: (2048, 512),\n",
              "                    },\n",
              "                },\n",
              "                MultiHeadDotProductAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "                SelfAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "            },\n",
              "            encoderdecoderblock_2: {\n",
              "                LayerNorm_0: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_1: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_2: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                MlpBlock_0: {\n",
              "                    Dense_0: {\n",
              "                        bias: (2048,),\n",
              "                        kernel: (512, 2048),\n",
              "                    },\n",
              "                    Dense_1: {\n",
              "                        bias: (512,),\n",
              "                        kernel: (2048, 512),\n",
              "                    },\n",
              "                },\n",
              "                MultiHeadDotProductAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "                SelfAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "            },\n",
              "            encoderdecoderblock_3: {\n",
              "                LayerNorm_0: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_1: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_2: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                MlpBlock_0: {\n",
              "                    Dense_0: {\n",
              "                        bias: (2048,),\n",
              "                        kernel: (512, 2048),\n",
              "                    },\n",
              "                    Dense_1: {\n",
              "                        bias: (512,),\n",
              "                        kernel: (2048, 512),\n",
              "                    },\n",
              "                },\n",
              "                MultiHeadDotProductAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "                SelfAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "            },\n",
              "            encoderdecoderblock_4: {\n",
              "                LayerNorm_0: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_1: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_2: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                MlpBlock_0: {\n",
              "                    Dense_0: {\n",
              "                        bias: (2048,),\n",
              "                        kernel: (512, 2048),\n",
              "                    },\n",
              "                    Dense_1: {\n",
              "                        bias: (512,),\n",
              "                        kernel: (2048, 512),\n",
              "                    },\n",
              "                },\n",
              "                MultiHeadDotProductAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "                SelfAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "            },\n",
              "            encoderdecoderblock_5: {\n",
              "                LayerNorm_0: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_1: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_2: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                MlpBlock_0: {\n",
              "                    Dense_0: {\n",
              "                        bias: (2048,),\n",
              "                        kernel: (512, 2048),\n",
              "                    },\n",
              "                    Dense_1: {\n",
              "                        bias: (512,),\n",
              "                        kernel: (2048, 512),\n",
              "                    },\n",
              "                },\n",
              "                MultiHeadDotProductAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "                SelfAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "            },\n",
              "            logitdense: {\n",
              "                bias: (7690,),\n",
              "                kernel: (512, 7690),\n",
              "            },\n",
              "        },\n",
              "        encoder: {\n",
              "            encoder_norm: {\n",
              "                bias: (512,),\n",
              "                scale: (512,),\n",
              "            },\n",
              "            encoderblock_0: {\n",
              "                LayerNorm_0: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_1: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                MlpBlock_0: {\n",
              "                    Dense_0: {\n",
              "                        bias: (2048,),\n",
              "                        kernel: (512, 2048),\n",
              "                    },\n",
              "                    Dense_1: {\n",
              "                        bias: (512,),\n",
              "                        kernel: (2048, 512),\n",
              "                    },\n",
              "                },\n",
              "                SelfAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "            },\n",
              "            encoderblock_1: {\n",
              "                LayerNorm_0: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_1: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                MlpBlock_0: {\n",
              "                    Dense_0: {\n",
              "                        bias: (2048,),\n",
              "                        kernel: (512, 2048),\n",
              "                    },\n",
              "                    Dense_1: {\n",
              "                        bias: (512,),\n",
              "                        kernel: (2048, 512),\n",
              "                    },\n",
              "                },\n",
              "                SelfAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "            },\n",
              "            encoderblock_2: {\n",
              "                LayerNorm_0: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_1: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                MlpBlock_0: {\n",
              "                    Dense_0: {\n",
              "                        bias: (2048,),\n",
              "                        kernel: (512, 2048),\n",
              "                    },\n",
              "                    Dense_1: {\n",
              "                        bias: (512,),\n",
              "                        kernel: (2048, 512),\n",
              "                    },\n",
              "                },\n",
              "                SelfAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "            },\n",
              "            encoderblock_3: {\n",
              "                LayerNorm_0: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_1: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                MlpBlock_0: {\n",
              "                    Dense_0: {\n",
              "                        bias: (2048,),\n",
              "                        kernel: (512, 2048),\n",
              "                    },\n",
              "                    Dense_1: {\n",
              "                        bias: (512,),\n",
              "                        kernel: (2048, 512),\n",
              "                    },\n",
              "                },\n",
              "                SelfAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "            },\n",
              "            encoderblock_4: {\n",
              "                LayerNorm_0: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_1: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                MlpBlock_0: {\n",
              "                    Dense_0: {\n",
              "                        bias: (2048,),\n",
              "                        kernel: (512, 2048),\n",
              "                    },\n",
              "                    Dense_1: {\n",
              "                        bias: (512,),\n",
              "                        kernel: (2048, 512),\n",
              "                    },\n",
              "                },\n",
              "                SelfAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "            },\n",
              "            encoderblock_5: {\n",
              "                LayerNorm_0: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                LayerNorm_1: {\n",
              "                    bias: (512,),\n",
              "                    scale: (512,),\n",
              "                },\n",
              "                MlpBlock_0: {\n",
              "                    Dense_0: {\n",
              "                        bias: (2048,),\n",
              "                        kernel: (512, 2048),\n",
              "                    },\n",
              "                    Dense_1: {\n",
              "                        bias: (512,),\n",
              "                        kernel: (2048, 512),\n",
              "                    },\n",
              "                },\n",
              "                SelfAttention_0: {\n",
              "                    key: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    out: {\n",
              "                        kernel: (8, 64, 512),\n",
              "                    },\n",
              "                    query: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                    value: {\n",
              "                        kernel: (512, 8, 64),\n",
              "                    },\n",
              "                },\n",
              "            },\n",
              "        },\n",
              "        shared_embedding: {\n",
              "            embedding: (7690, 512),\n",
              "        },\n",
              "    },\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn.tabulate(model, rngs={'params': init_rng, 'dropout': dropout_rng})(x, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "0UFSm-QHqBlO",
        "outputId": "dee89360-2151-4f4d-9370-e6b1894a466c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-0ae42f95dc7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtabulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrngs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minit_rng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dropout'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdropout_rng\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/flax/linen/summary.py\u001b[0m in \u001b[0;36m_tabulate_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m                                  \u001b[0mmutable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmutable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                                  exclude_methods=set(exclude_methods))\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_render_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/flax/linen/summary.py\u001b[0m in \u001b[0;36m_get_table_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0mvariables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape_variables\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             output_methods=output_methods))\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/flax/linen/summary.py\u001b[0m in \u001b[0;36m_flatten_to_rows\u001b[0;34m(path, variables, depth, output_methods)\u001b[0m\n\u001b[1;32m    254\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_outputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     raise ValueError(\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0;34mf\"Cannot infer output, module '{'/'.join(path)}' has multiple \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m         \u001b[0;34mf\"intermediates: {list(module_outputs.keys())}. Use the `exclude_methods` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         f\"argument to make sure each module only reports one output.\")\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot infer output, module '' has multiple intermediates: ['__call__', 'decode', 'encode']. Use the `exclude_methods` argument to make sure each module only reports one output."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* exclude_methods가 필요하다!\n",
        "\n",
        "* 어떻게 필요하냐면...\n",
        "* multiple intermediates라고 한다. `['__call__', 'decode', 'encode']`가 있고, `'__call__'`을 제외한 다른 intermediates를 exclude_methods에 넣어야 한다. \n",
        "* Transformer class안에 `encode`라는 함수와 `decode`라는 함수, 그리고 `__call__`이라는 함수가 있는데, 우리는 `__call__`이 필요하니 나머지 함수들은 메소드들은 지우는 것이다. \n",
        "\n",
        "```python\n",
        "exclude_methods=['encode','decode']\n",
        "```"
      ],
      "metadata": {
        "id": "8y9SbzeRqDc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn.tabulate(model, rngs={'params': init_rng, 'dropout': dropout_rng}, exclude_methods=['encode','decode'])(x, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BgAC6Oe2kYF6",
        "outputId": "2ddc0860-5756-4bc5-f851-2a54f0c83178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[3m                                     Transformer Summary                                     \u001b[0m\n",
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mpath                               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams                      \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ Inputs                              │ - \u001b[2mbfloat16\u001b[0m[128,15]   │                              │\n",
              "│                                     │ - \u001b[2mbfloat16\u001b[0m[128,15]   │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/Dropout_0                   │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoder_norm         │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ scale: \u001b[2mfloat32\u001b[0m[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/Drop… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/Drop… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/Laye… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ scale: \u001b[2mfloat32\u001b[0m[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/Laye… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ scale: \u001b[2mfloat32\u001b[0m[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/Laye… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ scale: \u001b[2mfloat32\u001b[0m[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/MlpB… │ \u001b[2mfloat32\u001b[0m[128,15,2048] │ bias: \u001b[2mfloat32\u001b[0m[2048]          │\n",
              "│                                     │                      │ kernel: \u001b[2mfloat32\u001b[0m[512,2048]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,050,624 \u001b[0m\u001b[1;2m(4.2 MB)\u001b[0m           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/MlpB… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ kernel: \u001b[2mfloat32\u001b[0m[2048,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,049,088 \u001b[0m\u001b[1;2m(4.2 MB)\u001b[0m           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/MlpB… │ \u001b[2mfloat32\u001b[0m[128,15,2048] │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/MlpB… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/MlpB… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/Mult… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/Mult… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ kernel: \u001b[2mfloat32\u001b[0m[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/Mult… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/Mult… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/Mult… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/Self… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/Self… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ kernel: \u001b[2mfloat32\u001b[0m[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/Self… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/Self… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/Self… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0       │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/Drop… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/Drop… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/Laye… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ scale: \u001b[2mfloat32\u001b[0m[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/Laye… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ scale: \u001b[2mfloat32\u001b[0m[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/Laye… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ scale: \u001b[2mfloat32\u001b[0m[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/MlpB… │ \u001b[2mfloat32\u001b[0m[128,15,2048] │ bias: \u001b[2mfloat32\u001b[0m[2048]          │\n",
              "│                                     │                      │ kernel: \u001b[2mfloat32\u001b[0m[512,2048]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,050,624 \u001b[0m\u001b[1;2m(4.2 MB)\u001b[0m           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/MlpB… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ kernel: \u001b[2mfloat32\u001b[0m[2048,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,049,088 \u001b[0m\u001b[1;2m(4.2 MB)\u001b[0m           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/MlpB… │ \u001b[2mfloat32\u001b[0m[128,15,2048] │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/MlpB… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/MlpB… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/Mult… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/Mult… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ kernel: \u001b[2mfloat32\u001b[0m[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/Mult… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/Mult… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/Mult… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/Self… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/Self… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ kernel: \u001b[2mfloat32\u001b[0m[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/Self… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/Self… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/Self… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1       │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/Drop… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/Drop… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/Laye… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ scale: \u001b[2mfloat32\u001b[0m[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/Laye… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ scale: \u001b[2mfloat32\u001b[0m[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/Laye… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ scale: \u001b[2mfloat32\u001b[0m[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/MlpB… │ \u001b[2mfloat32\u001b[0m[128,15,2048] │ bias: \u001b[2mfloat32\u001b[0m[2048]          │\n",
              "│                                     │                      │ kernel: \u001b[2mfloat32\u001b[0m[512,2048]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,050,624 \u001b[0m\u001b[1;2m(4.2 MB)\u001b[0m           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/MlpB… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ kernel: \u001b[2mfloat32\u001b[0m[2048,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,049,088 \u001b[0m\u001b[1;2m(4.2 MB)\u001b[0m           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/MlpB… │ \u001b[2mfloat32\u001b[0m[128,15,2048] │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/MlpB… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/MlpB… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/Mult… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/Mult… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ kernel: \u001b[2mfloat32\u001b[0m[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/Mult… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/Mult… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/Mult… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/Self… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/Self… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ kernel: \u001b[2mfloat32\u001b[0m[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/Self… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/Self… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/Self… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2       │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/Drop… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/Drop… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/Laye… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ scale: \u001b[2mfloat32\u001b[0m[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/Laye… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ scale: \u001b[2mfloat32\u001b[0m[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/Laye… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ scale: \u001b[2mfloat32\u001b[0m[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/MlpB… │ \u001b[2mfloat32\u001b[0m[128,15,2048] │ bias: \u001b[2mfloat32\u001b[0m[2048]          │\n",
              "│                                     │                      │ kernel: \u001b[2mfloat32\u001b[0m[512,2048]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,050,624 \u001b[0m\u001b[1;2m(4.2 MB)\u001b[0m           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/MlpB… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ kernel: \u001b[2mfloat32\u001b[0m[2048,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,049,088 \u001b[0m\u001b[1;2m(4.2 MB)\u001b[0m           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/MlpB… │ \u001b[2mfloat32\u001b[0m[128,15,2048] │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/MlpB… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/MlpB… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/Mult… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/Mult… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ kernel: \u001b[2mfloat32\u001b[0m[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/Mult… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/Mult… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/Mult… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/Self… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/Self… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ kernel: \u001b[2mfloat32\u001b[0m[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/Self… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/Self… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/Self… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3       │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/Drop… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/Drop… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/Laye… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ scale: \u001b[2mfloat32\u001b[0m[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/Laye… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ scale: \u001b[2mfloat32\u001b[0m[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/Laye… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ scale: \u001b[2mfloat32\u001b[0m[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/MlpB… │ \u001b[2mfloat32\u001b[0m[128,15,2048] │ bias: \u001b[2mfloat32\u001b[0m[2048]          │\n",
              "│                                     │                      │ kernel: \u001b[2mfloat32\u001b[0m[512,2048]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,050,624 \u001b[0m\u001b[1;2m(4.2 MB)\u001b[0m           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/MlpB… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ kernel: \u001b[2mfloat32\u001b[0m[2048,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,049,088 \u001b[0m\u001b[1;2m(4.2 MB)\u001b[0m           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/MlpB… │ \u001b[2mfloat32\u001b[0m[128,15,2048] │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/MlpB… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/MlpB… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/Mult… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/Mult… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ kernel: \u001b[2mfloat32\u001b[0m[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/Mult… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/Mult… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/Mult… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/Self… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/Self… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ kernel: \u001b[2mfloat32\u001b[0m[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/Self… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/Self… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/Self… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4       │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/Drop… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/Drop… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/Laye… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ scale: \u001b[2mfloat32\u001b[0m[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/Laye… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ scale: \u001b[2mfloat32\u001b[0m[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/Laye… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ scale: \u001b[2mfloat32\u001b[0m[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/MlpB… │ \u001b[2mfloat32\u001b[0m[128,15,2048] │ bias: \u001b[2mfloat32\u001b[0m[2048]          │\n",
              "│                                     │                      │ kernel: \u001b[2mfloat32\u001b[0m[512,2048]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,050,624 \u001b[0m\u001b[1;2m(4.2 MB)\u001b[0m           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/MlpB… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ kernel: \u001b[2mfloat32\u001b[0m[2048,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,049,088 \u001b[0m\u001b[1;2m(4.2 MB)\u001b[0m           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/MlpB… │ \u001b[2mfloat32\u001b[0m[128,15,2048] │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/MlpB… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/MlpB… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/Mult… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/Mult… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ kernel: \u001b[2mfloat32\u001b[0m[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/Mult… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/Mult… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/Mult… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/Self… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/Self… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ kernel: \u001b[2mfloat32\u001b[0m[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/Self… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/Self… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/Self… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5       │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/logitdense                  │ \u001b[2mfloat32\u001b[0m[128,15,7690] │ bias: \u001b[2mfloat32\u001b[0m[7690]          │\n",
              "│                                     │                      │ kernel: \u001b[2mfloat32\u001b[0m[512,7690]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m3,944,970 \u001b[0m\u001b[1;2m(15.8 MB)\u001b[0m          │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/posembed_output             │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder                             │ \u001b[2mfloat32\u001b[0m[128,15,7690] │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/Dropout_0                   │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoder_norm                │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ scale: \u001b[2mfloat32\u001b[0m[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_0/Dropout_0    │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_0/LayerNorm_0  │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ scale: \u001b[2mfloat32\u001b[0m[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_0/LayerNorm_1  │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ scale: \u001b[2mfloat32\u001b[0m[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_0/MlpBlock_0/… │ \u001b[2mfloat32\u001b[0m[128,15,2048] │ bias: \u001b[2mfloat32\u001b[0m[2048]          │\n",
              "│                                     │                      │ kernel: \u001b[2mfloat32\u001b[0m[512,2048]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,050,624 \u001b[0m\u001b[1;2m(4.2 MB)\u001b[0m           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_0/MlpBlock_0/… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ kernel: \u001b[2mfloat32\u001b[0m[2048,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,049,088 \u001b[0m\u001b[1;2m(4.2 MB)\u001b[0m           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_0/MlpBlock_0/… │ \u001b[2mfloat32\u001b[0m[128,15,2048] │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_0/MlpBlock_0/… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_0/MlpBlock_0   │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_0/SelfAttenti… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_0/SelfAttenti… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ kernel: \u001b[2mfloat32\u001b[0m[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_0/SelfAttenti… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_0/SelfAttenti… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_0/SelfAttenti… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_0              │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_1/Dropout_0    │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_1/LayerNorm_0  │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ scale: \u001b[2mfloat32\u001b[0m[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_1/LayerNorm_1  │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ scale: \u001b[2mfloat32\u001b[0m[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_1/MlpBlock_0/… │ \u001b[2mfloat32\u001b[0m[128,15,2048] │ bias: \u001b[2mfloat32\u001b[0m[2048]          │\n",
              "│                                     │                      │ kernel: \u001b[2mfloat32\u001b[0m[512,2048]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,050,624 \u001b[0m\u001b[1;2m(4.2 MB)\u001b[0m           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_1/MlpBlock_0/… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ kernel: \u001b[2mfloat32\u001b[0m[2048,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,049,088 \u001b[0m\u001b[1;2m(4.2 MB)\u001b[0m           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_1/MlpBlock_0/… │ \u001b[2mfloat32\u001b[0m[128,15,2048] │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_1/MlpBlock_0/… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_1/MlpBlock_0   │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_1/SelfAttenti… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_1/SelfAttenti… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ kernel: \u001b[2mfloat32\u001b[0m[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_1/SelfAttenti… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_1/SelfAttenti… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_1/SelfAttenti… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_1              │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_2/Dropout_0    │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_2/LayerNorm_0  │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ scale: \u001b[2mfloat32\u001b[0m[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_2/LayerNorm_1  │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ scale: \u001b[2mfloat32\u001b[0m[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_2/MlpBlock_0/… │ \u001b[2mfloat32\u001b[0m[128,15,2048] │ bias: \u001b[2mfloat32\u001b[0m[2048]          │\n",
              "│                                     │                      │ kernel: \u001b[2mfloat32\u001b[0m[512,2048]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,050,624 \u001b[0m\u001b[1;2m(4.2 MB)\u001b[0m           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_2/MlpBlock_0/… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ kernel: \u001b[2mfloat32\u001b[0m[2048,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,049,088 \u001b[0m\u001b[1;2m(4.2 MB)\u001b[0m           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_2/MlpBlock_0/… │ \u001b[2mfloat32\u001b[0m[128,15,2048] │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_2/MlpBlock_0/… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_2/MlpBlock_0   │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_2/SelfAttenti… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_2/SelfAttenti… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ kernel: \u001b[2mfloat32\u001b[0m[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_2/SelfAttenti… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_2/SelfAttenti… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_2/SelfAttenti… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_2              │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_3/Dropout_0    │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_3/LayerNorm_0  │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ scale: \u001b[2mfloat32\u001b[0m[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_3/LayerNorm_1  │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ scale: \u001b[2mfloat32\u001b[0m[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_3/MlpBlock_0/… │ \u001b[2mfloat32\u001b[0m[128,15,2048] │ bias: \u001b[2mfloat32\u001b[0m[2048]          │\n",
              "│                                     │                      │ kernel: \u001b[2mfloat32\u001b[0m[512,2048]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,050,624 \u001b[0m\u001b[1;2m(4.2 MB)\u001b[0m           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_3/MlpBlock_0/… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ kernel: \u001b[2mfloat32\u001b[0m[2048,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,049,088 \u001b[0m\u001b[1;2m(4.2 MB)\u001b[0m           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_3/MlpBlock_0/… │ \u001b[2mfloat32\u001b[0m[128,15,2048] │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_3/MlpBlock_0/… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_3/MlpBlock_0   │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_3/SelfAttenti… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_3/SelfAttenti… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ kernel: \u001b[2mfloat32\u001b[0m[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_3/SelfAttenti… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_3/SelfAttenti… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_3/SelfAttenti… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_3              │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_4/Dropout_0    │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_4/LayerNorm_0  │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ scale: \u001b[2mfloat32\u001b[0m[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_4/LayerNorm_1  │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ scale: \u001b[2mfloat32\u001b[0m[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_4/MlpBlock_0/… │ \u001b[2mfloat32\u001b[0m[128,15,2048] │ bias: \u001b[2mfloat32\u001b[0m[2048]          │\n",
              "│                                     │                      │ kernel: \u001b[2mfloat32\u001b[0m[512,2048]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,050,624 \u001b[0m\u001b[1;2m(4.2 MB)\u001b[0m           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_4/MlpBlock_0/… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ kernel: \u001b[2mfloat32\u001b[0m[2048,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,049,088 \u001b[0m\u001b[1;2m(4.2 MB)\u001b[0m           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_4/MlpBlock_0/… │ \u001b[2mfloat32\u001b[0m[128,15,2048] │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_4/MlpBlock_0/… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_4/MlpBlock_0   │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_4/SelfAttenti… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_4/SelfAttenti… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ kernel: \u001b[2mfloat32\u001b[0m[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_4/SelfAttenti… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_4/SelfAttenti… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_4/SelfAttenti… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_4              │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_5/Dropout_0    │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_5/LayerNorm_0  │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ scale: \u001b[2mfloat32\u001b[0m[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_5/LayerNorm_1  │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ scale: \u001b[2mfloat32\u001b[0m[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_5/MlpBlock_0/… │ \u001b[2mfloat32\u001b[0m[128,15,2048] │ bias: \u001b[2mfloat32\u001b[0m[2048]          │\n",
              "│                                     │                      │ kernel: \u001b[2mfloat32\u001b[0m[512,2048]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,050,624 \u001b[0m\u001b[1;2m(4.2 MB)\u001b[0m           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_5/MlpBlock_0/… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                                     │                      │ kernel: \u001b[2mfloat32\u001b[0m[2048,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m1,049,088 \u001b[0m\u001b[1;2m(4.2 MB)\u001b[0m           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_5/MlpBlock_0/… │ \u001b[2mfloat32\u001b[0m[128,15,2048] │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_5/MlpBlock_0/… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_5/MlpBlock_0   │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_5/SelfAttenti… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_5/SelfAttenti… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ kernel: \u001b[2mfloat32\u001b[0m[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_5/SelfAttenti… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_5/SelfAttenti… │ \u001b[2mfloat32\u001b[0m[128,15,8,64] │ kernel: \u001b[2mfloat32\u001b[0m[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m262,144 \u001b[0m\u001b[1;2m(1.0 MB)\u001b[0m             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_5/SelfAttenti… │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_5              │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/posembed_input              │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder                             │ \u001b[2mfloat32\u001b[0m[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ shared_embedding                    │ \u001b[2mfloat32\u001b[0m[128,15,512]  │ embedding: \u001b[2mfloat32\u001b[0m[7690,512] │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ \u001b[1m3,937,280 \u001b[0m\u001b[1;2m(15.7 MB)\u001b[0m          │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ Transformer                         │ \u001b[2mfloat32\u001b[0m[128,15,7690] │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│\u001b[1m \u001b[0m\u001b[1m                                   \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m               Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m51,985,930 \u001b[0m\u001b[1;2m(207.9 MB)\u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m│\n",
              "└─────────────────────────────────────┴──────────────────────┴──────────────────────────────┘\n",
              "\u001b[1m                                                                                             \u001b[0m\n",
              "\u001b[1m                           Total Parameters: 51,985,930 \u001b[0m\u001b[1;2m(207.9 MB)\u001b[0m\u001b[1m                           \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                     Transformer Summary                                     </span>\n",
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> path                                </span>┃<span style=\"font-weight: bold\"> outputs              </span>┃<span style=\"font-weight: bold\"> params                       </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ Inputs                              │ - <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">bfloat16</span>[128,15]   │                              │\n",
              "│                                     │ - <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">bfloat16</span>[128,15]   │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/Dropout_0                   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoder_norm         │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/Drop… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/Drop… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/Laye… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/Laye… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/Laye… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/MlpB… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,2048] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2048]          │\n",
              "│                                     │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,2048]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,050,624 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.2 MB)</span>           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/MlpB… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2048,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,049,088 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.2 MB)</span>           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/MlpB… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,2048] │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/MlpB… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/MlpB… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/Mult… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/Mult… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/Mult… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/Mult… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/Mult… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/Self… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/Self… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/Self… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/Self… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0/Self… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_0       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/Drop… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/Drop… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/Laye… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/Laye… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/Laye… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/MlpB… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,2048] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2048]          │\n",
              "│                                     │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,2048]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,050,624 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.2 MB)</span>           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/MlpB… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2048,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,049,088 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.2 MB)</span>           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/MlpB… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,2048] │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/MlpB… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/MlpB… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/Mult… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/Mult… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/Mult… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/Mult… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/Mult… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/Self… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/Self… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/Self… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/Self… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1/Self… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_1       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/Drop… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/Drop… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/Laye… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/Laye… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/Laye… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/MlpB… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,2048] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2048]          │\n",
              "│                                     │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,2048]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,050,624 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.2 MB)</span>           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/MlpB… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2048,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,049,088 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.2 MB)</span>           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/MlpB… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,2048] │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/MlpB… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/MlpB… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/Mult… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/Mult… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/Mult… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/Mult… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/Mult… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/Self… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/Self… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/Self… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/Self… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2/Self… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_2       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/Drop… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/Drop… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/Laye… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/Laye… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/Laye… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/MlpB… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,2048] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2048]          │\n",
              "│                                     │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,2048]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,050,624 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.2 MB)</span>           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/MlpB… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2048,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,049,088 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.2 MB)</span>           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/MlpB… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,2048] │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/MlpB… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/MlpB… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/Mult… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/Mult… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/Mult… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/Mult… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/Mult… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/Self… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/Self… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/Self… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/Self… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3/Self… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_3       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/Drop… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/Drop… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/Laye… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/Laye… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/Laye… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/MlpB… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,2048] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2048]          │\n",
              "│                                     │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,2048]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,050,624 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.2 MB)</span>           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/MlpB… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2048,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,049,088 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.2 MB)</span>           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/MlpB… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,2048] │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/MlpB… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/MlpB… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/Mult… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/Mult… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/Mult… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/Mult… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/Mult… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/Self… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/Self… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/Self… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/Self… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4/Self… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_4       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/Drop… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/Drop… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/Laye… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/Laye… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/Laye… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/MlpB… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,2048] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2048]          │\n",
              "│                                     │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,2048]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,050,624 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.2 MB)</span>           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/MlpB… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2048,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,049,088 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.2 MB)</span>           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/MlpB… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,2048] │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/MlpB… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/MlpB… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/Mult… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/Mult… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/Mult… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/Mult… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/Mult… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/Self… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/Self… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/Self… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/Self… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5/Self… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/encoderdecoderblock_5       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/logitdense                  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,7690] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[7690]          │\n",
              "│                                     │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,7690]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">3,944,970 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(15.8 MB)</span>          │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder/posembed_output             │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ decoder                             │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,7690] │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/Dropout_0                   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoder_norm                │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_0/Dropout_0    │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_0/LayerNorm_0  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_0/LayerNorm_1  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_0/MlpBlock_0/… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,2048] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2048]          │\n",
              "│                                     │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,2048]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,050,624 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.2 MB)</span>           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_0/MlpBlock_0/… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2048,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,049,088 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.2 MB)</span>           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_0/MlpBlock_0/… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,2048] │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_0/MlpBlock_0/… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_0/MlpBlock_0   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_0/SelfAttenti… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_0/SelfAttenti… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_0/SelfAttenti… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_0/SelfAttenti… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_0/SelfAttenti… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_0              │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_1/Dropout_0    │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_1/LayerNorm_0  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_1/LayerNorm_1  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_1/MlpBlock_0/… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,2048] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2048]          │\n",
              "│                                     │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,2048]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,050,624 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.2 MB)</span>           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_1/MlpBlock_0/… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2048,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,049,088 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.2 MB)</span>           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_1/MlpBlock_0/… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,2048] │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_1/MlpBlock_0/… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_1/MlpBlock_0   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_1/SelfAttenti… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_1/SelfAttenti… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_1/SelfAttenti… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_1/SelfAttenti… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_1/SelfAttenti… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_1              │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_2/Dropout_0    │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_2/LayerNorm_0  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_2/LayerNorm_1  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_2/MlpBlock_0/… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,2048] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2048]          │\n",
              "│                                     │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,2048]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,050,624 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.2 MB)</span>           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_2/MlpBlock_0/… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2048,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,049,088 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.2 MB)</span>           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_2/MlpBlock_0/… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,2048] │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_2/MlpBlock_0/… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_2/MlpBlock_0   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_2/SelfAttenti… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_2/SelfAttenti… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_2/SelfAttenti… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_2/SelfAttenti… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_2/SelfAttenti… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_2              │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_3/Dropout_0    │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_3/LayerNorm_0  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_3/LayerNorm_1  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_3/MlpBlock_0/… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,2048] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2048]          │\n",
              "│                                     │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,2048]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,050,624 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.2 MB)</span>           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_3/MlpBlock_0/… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2048,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,049,088 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.2 MB)</span>           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_3/MlpBlock_0/… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,2048] │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_3/MlpBlock_0/… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_3/MlpBlock_0   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_3/SelfAttenti… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_3/SelfAttenti… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_3/SelfAttenti… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_3/SelfAttenti… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_3/SelfAttenti… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_3              │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_4/Dropout_0    │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_4/LayerNorm_0  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_4/LayerNorm_1  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_4/MlpBlock_0/… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,2048] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2048]          │\n",
              "│                                     │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,2048]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,050,624 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.2 MB)</span>           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_4/MlpBlock_0/… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2048,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,049,088 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.2 MB)</span>           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_4/MlpBlock_0/… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,2048] │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_4/MlpBlock_0/… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_4/MlpBlock_0   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_4/SelfAttenti… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_4/SelfAttenti… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_4/SelfAttenti… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_4/SelfAttenti… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_4/SelfAttenti… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_4              │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_5/Dropout_0    │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_5/LayerNorm_0  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_5/LayerNorm_1  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]          │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.1 KB)</span>               │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_5/MlpBlock_0/… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,2048] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2048]          │\n",
              "│                                     │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,2048]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,050,624 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.2 MB)</span>           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_5/MlpBlock_0/… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                                     │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2048,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">1,049,088 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.2 MB)</span>           │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_5/MlpBlock_0/… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,2048] │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_5/MlpBlock_0/… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_5/MlpBlock_0   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_5/SelfAttenti… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_5/SelfAttenti… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[8,64,512]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_5/SelfAttenti… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_5/SelfAttenti… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,8,64] │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,8,64]    │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">262,144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.0 MB)</span>             │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_5/SelfAttenti… │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/encoderblock_5              │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder/posembed_input              │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ encoder                             │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ shared_embedding                    │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,512]  │ embedding: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[7690,512] │\n",
              "│                                     │                      │                              │\n",
              "│                                     │                      │ <span style=\"font-weight: bold\">3,937,280 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(15.7 MB)</span>          │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ Transformer                         │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,15,7690] │                              │\n",
              "├─────────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│<span style=\"font-weight: bold\">                                     </span>│<span style=\"font-weight: bold\">                Total </span>│<span style=\"font-weight: bold\"> 51,985,930 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(207.9 MB)</span><span style=\"font-weight: bold\">        </span>│\n",
              "└─────────────────────────────────────┴──────────────────────┴──────────────────────────────┘\n",
              "<span style=\"font-weight: bold\">                                                                                             </span>\n",
              "<span style=\"font-weight: bold\">                           Total Parameters: 51,985,930 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(207.9 MB)</span><span style=\"font-weight: bold\">                           </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class QADataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, vocab_size, max_len, src, tgt):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_len = max_len\n",
        "        self.src = src\n",
        "        self.tgt = tgt\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src = self.src[idx]\n",
        "        tgt = self.tgt[idx]\n",
        "        return src, tgt"
      ],
      "metadata": {
        "id": "nnEi2t-N7E-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def numpy_collate(batch):\n",
        "    if isinstance(batch[0], np.ndarray):\n",
        "        return np.stack(batch)\n",
        "    elif isinstance(batch[0], (tuple,list)):\n",
        "        transposed = zip(*batch)\n",
        "        return [numpy_collate(samples) for samples in transposed]\n",
        "    else:\n",
        "        return np.array(batch)\n",
        "\n",
        "\n",
        "qa_train_loader = data.DataLoader(QADataset(vocab_size=len(src_word_index), max_len=len(question_tensor[0]),src=question_tensor, tgt=answer_tensor),\n",
        "                                   batch_size=128,\n",
        "                                   shuffle=True,\n",
        "                                   drop_last=True,\n",
        "                                   collate_fn=numpy_collate)"
      ],
      "metadata": {
        "id": "7u_ff3oj7Pi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rsqrt_schedule(\n",
        "    init_value: float,\n",
        "    shift: int = 0,\n",
        "):\n",
        "  \"\"\"Applies a reverse square-root schedule.\n",
        "\n",
        "  The reverse square root schedule is simply `lr = init_value / sqrt(step)`.\n",
        "\n",
        "  Args:\n",
        "    init_value: Base learning rate (before applying the rsqrt schedule).\n",
        "    shift: How many steps the rsqrt should be shifted. Shifting the rsqrt\n",
        "      schedule makes it less steep in the beginning (close to 0).\n",
        "\n",
        "  Returns:\n",
        "    A schedule `count -> learning_rate`.\n",
        "  \"\"\"\n",
        "\n",
        "  def schedule(count):\n",
        "    return init_value * (count + shift)**-.5 * shift**.5\n",
        "\n",
        "  return schedule\n",
        "\n",
        "\n",
        "def create_learning_rate_schedule(learning_rate: float, warmup_steps: int):\n",
        "  \"\"\"Creates a rsqrt schedule with linear warmup.\"\"\"\n",
        "  return optax.join_schedules([\n",
        "      optax.linear_schedule(\n",
        "          init_value=0, end_value=learning_rate, transition_steps=warmup_steps),\n",
        "      rsqrt_schedule(init_value=learning_rate, shift=warmup_steps),\n",
        "  ],\n",
        "                              boundaries=[warmup_steps])\n",
        "\n",
        "\n",
        "def compute_weighted_cross_entropy(logits,\n",
        "                                   targets,\n",
        "                                   weights=None,\n",
        "                                   label_smoothing=0.0):\n",
        "  \"\"\"Compute weighted cross entropy and entropy for log probs and targets.\n",
        "\n",
        "  Args:\n",
        "   logits: [batch, length, num_classes] float array.\n",
        "   targets: categorical targets [batch, length] int array.\n",
        "   weights: None or array of shape [batch, length].\n",
        "   label_smoothing: label smoothing constant, used to determine the on and off\n",
        "     values.\n",
        "\n",
        "  Returns:\n",
        "    Tuple of scalar loss and batch normalizing factor.\n",
        "  \"\"\"\n",
        "  if logits.ndim != targets.ndim + 1:\n",
        "    raise ValueError(\"Incorrect shapes. Got shape %s logits and %s targets\" %\n",
        "                     (str(logits.shape), str(targets.shape)))\n",
        "  vocab_size = logits.shape[-1]\n",
        "  confidence = 1.0 - label_smoothing\n",
        "  low_confidence = (1.0 - confidence) / (vocab_size - 1)\n",
        "  normalizing_constant = -(\n",
        "      confidence * jnp.log(confidence) +\n",
        "      (vocab_size - 1) * low_confidence * jnp.log(low_confidence + 1e-20))\n",
        "  soft_targets = common_utils.onehot(\n",
        "      targets, vocab_size, on_value=confidence, off_value=low_confidence)\n",
        "\n",
        "  loss = -jnp.sum(soft_targets * nn.log_softmax(logits), axis=-1)\n",
        "  loss = loss - normalizing_constant\n",
        "\n",
        "  normalizing_factor = np.prod(targets.shape)\n",
        "  if weights is not None:\n",
        "    loss = loss * weights\n",
        "    normalizing_factor = weights.sum()\n",
        "\n",
        "  return loss.sum(), normalizing_factor\n",
        "\n",
        "\n",
        "def compute_weighted_accuracy(logits, targets, weights=None):\n",
        "  \"\"\"Compute weighted accuracy for log probs and targets.\n",
        "\n",
        "  Args:\n",
        "   logits: [batch, length, num_classes] float array.\n",
        "   targets: categorical targets [batch, length] int array.\n",
        "   weights: None or array of shape [batch, length]\n",
        "\n",
        "  Returns:\n",
        "    Tuple of scalar loss and batch normalizing factor.\n",
        "  \"\"\"\n",
        "  if logits.ndim != targets.ndim + 1:\n",
        "    raise ValueError(\"Incorrect shapes. Got shape %s logits and %s targets\" %\n",
        "                     (str(logits.shape), str(targets.shape)))\n",
        "  loss = jnp.equal(jnp.argmax(logits, axis=-1), targets)\n",
        "  normalizing_factor = np.prod(logits.shape[:-1])\n",
        "  if weights is not None:\n",
        "    loss = loss * weights\n",
        "    normalizing_factor = weights.sum()\n",
        "\n",
        "  return loss.sum(), normalizing_factor\n",
        "\n",
        "\n",
        "def compute_metrics(logits, labels, weights, label_smoothing=0.0):\n",
        "  \"\"\"Compute summary metrics.\"\"\"\n",
        "  loss, weight_sum = compute_weighted_cross_entropy(logits, labels, weights,\n",
        "                                                    label_smoothing)\n",
        "  acc, _ = compute_weighted_accuracy(logits, labels, weights)\n",
        "  metrics = {\n",
        "      \"loss\": loss,\n",
        "      \"accuracy\": acc,\n",
        "      \"denominator\": weight_sum,\n",
        "  }\n",
        "  metrics = jax.lax.psum(metrics, axis_name=\"batch\")\n",
        "  return metrics\n",
        "\n",
        "\n",
        "# Primary training / eval / decode step functions.\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def train_step(state,\n",
        "               batch,\n",
        "               config,\n",
        "               learning_rate_fn,\n",
        "               label_smoothing=0.0,\n",
        "               dropout_rng=None):\n",
        "  \"\"\"Perform a single training step.\"\"\"\n",
        "  # X_position and X_segmentation are needed only when using \"packed examples\"\n",
        "  # where multiple sequences are packed into the same example with this\n",
        "  # metadata.\n",
        "  # if such features are not present they are ignored and the example is treated\n",
        "  # like a normal, unpacked sequence example.\n",
        "  (inputs, targets) = batch\n",
        "\n",
        "  weights = jnp.where(targets > 0, 1, 0).astype(jnp.float32)\n",
        "\n",
        "  dropout_rng = jax.random.fold_in(dropout_rng, state.step)\n",
        "\n",
        "  def loss_fn(params):\n",
        "    \"\"\"loss function used for training.\"\"\"\n",
        "    logits = Transformer(config).apply(\n",
        "        params,\n",
        "        inputs,\n",
        "        targets,\n",
        "        inputs_positions=None,\n",
        "        targets_positions=None,\n",
        "        inputs_segmentation=None,\n",
        "        targets_segmentation=None,\n",
        "        rngs={\"dropout\": dropout_rng})\n",
        "\n",
        "    loss, weight_sum = compute_weighted_cross_entropy(logits, targets, weights,\n",
        "                                                      label_smoothing)\n",
        "    mean_loss = loss / weight_sum\n",
        "    return mean_loss, logits\n",
        "  step = state.step\n",
        "\n",
        "\n",
        "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "  (_, logits), grads = grad_fn(state.params)\n",
        "  grads = jax.lax.pmean(grads, axis_name=\"batch\")\n",
        "  new_state = state.apply_gradients(grads=grads)\n",
        "  metrics = compute_metrics(logits, targets, weights)\n",
        "  metrics[\"learning_rate\"] = learning_rate_fn(step)\n",
        "  \n",
        "\n",
        "  return new_state, metrics, grads, logits, targets"
      ],
      "metadata": {
        "id": "kuZT2dcbevp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_weighted_accuracy(logits, targets, weights=None):\n",
        "  \"\"\"Compute weighted accuracy for log probs and targets.\n",
        "\n",
        "  Args:\n",
        "   logits: [batch, length, num_classes] float array.\n",
        "   targets: categorical targets [batch, length] int array.\n",
        "   weights: None or array of shape [batch, length]\n",
        "\n",
        "  Returns:\n",
        "    Tuple of scalar loss and batch normalizing factor.\n",
        "  \"\"\"\n",
        "  if logits.ndim != targets.ndim + 1:\n",
        "    raise ValueError(\"Incorrect shapes. Got shape %s logits and %s targets\" %\n",
        "                     (str(logits.shape), str(targets.shape)))\n",
        "  loss = jnp.equal(jnp.argmax(logits, axis=-1), targets)\n",
        "  normalizing_factor = np.prod(logits.shape[:-1])\n",
        "  if weights is not None:\n",
        "    loss = loss * weights\n",
        "    normalizing_factor = weights.sum()\n",
        "\n",
        "  return loss.sum(), normalizing_factor"
      ],
      "metadata": {
        "id": "LHYO21sHOs06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sc_8UhjET5b3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run\n"
      ],
      "metadata": {
        "id": "y3la37sZXEdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate_fn = create_learning_rate_schedule(\n",
        "    learning_rate=config.learning_rate, warmup_steps=config.warmup_steps)"
      ],
      "metadata": {
        "id": "qfncRtmrTjKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100"
      ],
      "metadata": {
        "id": "NyMB1vSubGAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_schedule = optax.warmup_cosine_decay_schedule(\n",
        "            init_value=0.0,\n",
        "            peak_value=0.02,\n",
        "            warmup_steps=100,\n",
        "            decay_steps=epochs*len(qa_train_loader),\n",
        "            end_value=0.0\n",
        "        )\n",
        "optimizer = optax.chain(\n",
        "    optax.clip_by_global_norm(1.0),  # Clip gradients at norm 1\n",
        "    optax.adam(lr_schedule)\n",
        ")"
      ],
      "metadata": {
        "id": "Dr-Ue6hKVw6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p_train_step = jax.pmap(\n",
        "    functools.partial(\n",
        "        train_step,\n",
        "        config=config,\n",
        "        learning_rate_fn=lr_schedule,\n",
        "        label_smoothing=config.label_smoothing),\n",
        "    axis_name=\"batch\",\n",
        "    donate_argnums=(0,)) "
      ],
      "metadata": {
        "id": "wGZSfL93CQB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = train_state.TrainState.create(apply_fn=model.apply, params=initial_variables, tx=optimizer)\n"
      ],
      "metadata": {
        "id": "AwAl0YDLVc7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* pmap에 넣으려면 state를 replicate를 해서 8개로 나누어야 한다. "
      ],
      "metadata": {
        "id": "k_cH5KLGlHkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state = flax.jax_utils.replicate(state,jax.local_devices())"
      ],
      "metadata": {
        "id": "SJzkaxBOiDMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* dropout에 필요한 `rng` (랜덤 키들)도 8개로 나누어 준다."
      ],
      "metadata": {
        "id": "tcMjAgr-axf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dropout_rngs = jax.random.split(rng, jax.local_device_count())"
      ],
      "metadata": {
        "id": "u3EQkDZSgCA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* pmap was requested to map its argument along axis 0, which implies that its rank should be at least 1, but is only 0 (its shape is ())\n",
        "\n",
        "* dropout rng 나 다른 모든 입력들은 pmap모양으로 나누어야 한다."
      ],
      "metadata": {
        "id": "2vSqRlOrXR1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = iter(qa_train_loader)\n",
        "batch = common_utils.shard(jax.tree_util.tree_map(np.asarray, next(train_loader)))\n",
        "state, metrics, grads, logits, targets = p_train_step(state, batch, dropout_rng=dropout_rngs)"
      ],
      "metadata": {
        "id": "q8Vcl17HUXo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqGjypcvUIDZ",
        "outputId": "867b8380-53e6-4f6e-b296-3044fe95b891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 16, 15, 7690)"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQXpF39rWyuD",
        "outputId": "1d2a25e9-befb-440b-82d5-0326c5218e09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 16, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6SsxOkZZVCw",
        "outputId": "2fb224c3-c273-45d1-e73d-99c7c6845613"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ShardedDeviceArray([[[ 290,   73, 1184, ...,    0,    0,    0],\n",
              "                     [ 764,    6, 1928, ...,    0,    0,    0],\n",
              "                     [   3,  397,  216, ...,    0,    0,    0],\n",
              "                     ...,\n",
              "                     [3563,  859,  721, ...,    0,    0,    0],\n",
              "                     [ 662,    3,   21, ...,    0,    0,    0],\n",
              "                     [  48, 2046,  352, ...,    0,    0,    0]],\n",
              "\n",
              "                    [[  64,    3,   39, ...,    0,    0,    0],\n",
              "                     [ 496,  779,   24, ...,    0,    0,    0],\n",
              "                     [ 658,   19,    3, ...,    0,    0,    0],\n",
              "                     ...,\n",
              "                     [ 223,   24,  553, ...,    0,    0,    0],\n",
              "                     [1096,   12,   13, ...,    0,    0,    0],\n",
              "                     [ 741,    2,  885, ...,    0,    0,    0]],\n",
              "\n",
              "                    [[ 598,    2,  543, ...,    0,    0,    0],\n",
              "                     [ 176,  336,    2, ...,    0,    0,    0],\n",
              "                     [  51,   96,   57, ...,    0,    0,    0],\n",
              "                     ...,\n",
              "                     [ 569, 2776,   12, ...,    0,    0,    0],\n",
              "                     [  35,  515,  473, ...,    0,    0,    0],\n",
              "                     [ 130,   19,  314, ...,    0,    0,    0]],\n",
              "\n",
              "                    ...,\n",
              "\n",
              "                    [[ 596,   96,   13, ...,    0,    0,    0],\n",
              "                     [ 221,   80,    4, ...,    0,    0,    0],\n",
              "                     [2387, 2518,   12, ...,    0,    0,    0],\n",
              "                     ...,\n",
              "                     [ 546,    6, 1760, ...,    0,    0,    0],\n",
              "                     [  50, 3881,   16, ...,    0,    0,    0],\n",
              "                     [  48,   24,  362, ...,    0,    0,    0]],\n",
              "\n",
              "                    [[ 103,   48,    6, ...,    0,    0,    0],\n",
              "                     [1055,  121,   67, ...,    0,    0,    0],\n",
              "                     [ 216,    4,  686, ...,    0,    0,    0],\n",
              "                     ...,\n",
              "                     [ 357,    6, 2792, ...,   10,   25,   22],\n",
              "                     [ 147,   49,   70, ...,    0,    0,    0],\n",
              "                     [ 371,   29,  408, ...,    0,    0,    0]],\n",
              "\n",
              "                    [[ 146,   72,   27, ...,    0,    0,    0],\n",
              "                     [1837,    6, 1065, ...,    0,    0,    0],\n",
              "                     [ 210,  274,   23, ...,    0,    0,    0],\n",
              "                     ...,\n",
              "                     [1847,  605,   22, ...,    0,    0,    0],\n",
              "                     [ 613,   34,  227, ...,    0,    0,    0],\n",
              "                     [  78,  101,  905, ...,    0,    0,    0]]],                   dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights = jnp.where(targets > 0, 1, 0).astype(jnp.float32)\n",
        "print(weights.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KRut25QYoYA",
        "outputId": "e27d3771-1221-44e7-98e6-5764cfcb904f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 16, 15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jnp.argmax(logits, axis=-1).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEb79KmzZ6Op",
        "outputId": "c5b09016-d49d-4eab-e82b-88bf57ce9a33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 16, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = jnp.equal(jnp.argmax(logits, axis=-1), targets)\n",
        "normalizing_factor = np.prod(logits.shape[:-1])\n",
        "print(loss)\n",
        "loss = loss * weights\n",
        "normalizing_factor = weights.sum()\n",
        "print(loss.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAq8GYWcYd05",
        "outputId": "4f747ec1-5665-4b35-fef2-18c50a4f4b78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  ...\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]]\n",
            "\n",
            " [[False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  ...\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]]\n",
            "\n",
            " [[False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  ...\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  ...\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]]\n",
            "\n",
            " [[False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  ...\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]]\n",
            "\n",
            " [[False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  ...\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]]]\n",
            "(8, 16, 15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSwhEc4YYznk",
        "outputId": "785ad43f-d4a7-4a4f-beb1-e8dabfdb2ea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray(0., dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "8*16*15"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rP8eSdQ2aQA7",
        "outputId": "8da8ae57-0657-4e70-f5de-0f5c66c91806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1920"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, weight_sum = compute_weighted_cross_entropy(logits, targets, weights,\n",
        "                                                    label_smoothing=0.1)"
      ],
      "metadata": {
        "id": "fL7g2E_Ba-8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train"
      ],
      "metadata": {
        "id": "7jQPHx00a_Wf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for step in range(epochs):\n",
        "    for batch_idx in tqdm(range(len(qa_train_loader))):\n",
        "        train_loader = iter(qa_train_loader)\n",
        "        batch = common_utils.shard(jax.tree_util.tree_map(np.asarray, next(train_loader)))\n",
        "        state, metrics, grads = p_train_step(state, batch, dropout_rng=dropout_rngs)\n",
        "        for key in metrics.keys():\n",
        "           metrics[key] = np.array(np.mean(metrics[key]))\n",
        "        wandb.log(metrics)\n",
        "        flatten_grad = frozen_dict_gradient_flatten(grads)\n",
        "        wandb_histo = {layers:wandb.Histogram(grads) for layers, grads in flatten_grad.items()}\n",
        "        wandb.log(wandb_histo)\n",
        "    print(f'TRAIN ({step}/{epochs}): metrics : {metrics}')\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_MK2qC-C05o",
        "outputId": "2102a495-9334-4e49-fef5-b0b346976feb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 258/258 [1:24:59<00:00, 19.76s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN (0/100): metrics : {'accuracy': array(246., dtype=float32), 'denominator': array(985., dtype=float32), 'learning_rate': array(0.01999814, dtype=float32), 'loss': array(4480.2627, dtype=float32)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 3/258 [01:11<1:41:15, 23.82s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key in metrics.keys():\n",
        "  metrics[key] = np.array(metrics[key])\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "sFDv-JDoNyRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Flax용 wandb gradients"
      ],
      "metadata": {
        "id": "qadktwd5s4k2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## wandb.keras.WandbCallback\n",
        "\n",
        "https://github.com/wandb/wandb/blob/0a3b035d0fb206570660275503c8b72f8d7b4399/wandb/integration/keras/keras.py#L934\n",
        "\n",
        "* 케라스용 wandb에서 그레디언트를 어떻게 시각화 하고 있는지 알아보도록 하자."
      ],
      "metadata": {
        "id": "CNn2BnExLomE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _log_gradients(self):\n",
        "    # Suppress callback warnings grad accumulator\n",
        "\n",
        "    self._grad_accumulator_model.fit(\n",
        "        self._training_data_x,\n",
        "        self._training_data_y,\n",
        "        verbose=0,\n",
        "        callbacks=[self._grad_accumulator_callback],\n",
        "    )\n",
        "\n",
        "    weights = self.model.trainable_weights\n",
        "    grads = self._grad_accumulator_callback.grads\n",
        "    metrics = {}\n",
        "    for (weight, grad) in zip(weights, grads):\n",
        "        metrics[\n",
        "            \"gradients/\" + weight.name.split(\":\")[0] + \".gradient\"\n",
        "        ] = wandb.Histogram(grad)\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "g-a4TTCtyvrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "c4gFI6fj0KLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class _GradAccumulatorCallback(tf.keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    Accumulates gradients during a fit() call when used in conjunction with\n",
        "    the CustomOptimizer above.\n",
        "    \"\"\"\n",
        "\n",
        "    def set_model(self, model):\n",
        "        super().set_model(model)\n",
        "        self.og_weights = model.get_weights()\n",
        "        self.grads = [np.zeros(tuple(w.shape)) for w in model.trainable_weights]\n",
        "\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        for g, w in zip(self.grads, self.model.trainable_weights):\n",
        "            g += w.numpy()\n",
        "        self.model.set_weights(self.og_weights)\n",
        "\n",
        "    def get_grads(self):\n",
        "        return [g.copy() for g in self.grads]"
      ],
      "metadata": {
        "id": "y35VN18Q0MrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_grad_accumulator_callback = _GradAccumulatorCallback()"
      ],
      "metadata": {
        "id": "XrInV8mR0NS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Dense(2, activation=\"relu\", name=\"layer1\"),\n",
        "        tf.keras.layers.Dense(3, activation=\"relu\", name=\"layer2\"),\n",
        "        tf.keras.layers.Dense(3, name=\"layer3\"),\n",
        "    ]\n",
        ")\n",
        "# Call model on a test input\n",
        "x = tf.random.uniform((3, 3),maxval=5)\n",
        "loss = tf.keras.losses.MeanSquaredError()\n",
        "a = [0,1,2]\n",
        "y_true = tf.convert_to_tensor([a,a,a])\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "\n",
        "\n",
        "model.build(input_shape=(3,3))\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "\n",
        "model.fit(x, y_true, verbose=0, callbacks=[_grad_accumulator_callback],)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6Fq5I-o0Y4S",
        "outputId": "15a76409-ad55-4018-db82-1f8edf012987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4a98ec3e90>"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = [0,1,3,4]\n",
        "print(y_true)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSjRDeKG6RrO",
        "outputId": "37ec805c-5365-4f4c-f90f-75b4a5bdcdf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 3, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_grad_accumulator_callback.grads"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkkVuhK347ry",
        "outputId": "ed29f026-ed0a-418c-98dc-a63c4c2d12dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[ 0.03186357, -0.71543443],\n",
              "        [-0.88569725,  0.74210143],\n",
              "        [-0.55287474,  0.83285022]]),\n",
              " array([ 0.        , -0.01328947]),\n",
              " array([[-0.5680694 ,  0.26485944, -0.36831582],\n",
              "        [ 0.43944037, -0.10591018, -0.66314888]]),\n",
              " array([-0.025164,  0.      ,  0.      ]),\n",
              " array([[-0.17332166,  0.63437182, -0.9452166 ],\n",
              "        [ 0.62420583, -0.67187238,  0.50105071],\n",
              "        [ 0.71027231, -0.93712902, -0.81261611]]),\n",
              " array([ 0.00202704, -0.00061873,  0.02465536])]"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grad = _grad_accumulator_callback.grads"
      ],
      "metadata": {
        "id": "wvHd_rlE7b2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = {}\n",
        "for n, i in enumerate(grad):\n",
        "  metric[str(n)] = wandb.Histogram(i)"
      ],
      "metadata": {
        "id": "fNQ8UNj87ZJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.log(metric)"
      ],
      "metadata": {
        "id": "UJaQANjP-acW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* array로 된 gradient값들을 딕셔너리로 층층이 `wandb.Histogram(grad)`에 담기게 된다. \n",
        "\n",
        "```python\n",
        ">>> print(metric)\n",
        "\n",
        "{'레이어 이름' : wandb.Histogram(grad),\n",
        "'레이어 이름' : wandb.Histogram(grad)....}\n",
        "```\n",
        "\n",
        "* 생성한 metric 딕셔너리를 wandb.log에 담으면 끝."
      ],
      "metadata": {
        "id": "BpxaAYvy9SU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric = {}\n",
        "for (weight, grad) in zip(weights, grads):\n",
        "            metrics[\n",
        "                \"gradients/\" + weight.name.split(\":\")[0] + \".gradient\"\n",
        "            ] = wandb.Histogram(grad)"
      ],
      "metadata": {
        "id": "fZlcIGWE7gg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flax의 그레디언트는?"
      ],
      "metadata": {
        "id": "_W8QNpABL-QJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def frozen_dict_gradient_flatten(some_dict):\n",
        "  flatten_dict = {}\n",
        "\n",
        "  def find_keys(some_dict, sub_key='Main'):\n",
        "\n",
        "    count = 0 \n",
        "    for key in some_dict.keys():\n",
        "      try:        \n",
        "\n",
        "        find_keys(some_dict[key], sub_key=f'{sub_key}/{key}')\n",
        "        count += 1\n",
        "      except:\n",
        "        path = f'{sub_key}/{key}'\n",
        "        flatten_dict[path] = some_dict[key]\n",
        "\n",
        "\n",
        "  find_keys(some_dict)\n",
        "\n",
        "  return flatten_dict"
      ],
      "metadata": {
        "id": "JGGCFI8mDyQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `train_step`의 pmap한 `p_train_step`함수에서 gradient를 전달하는 방식은 다음과 같다.\n",
        "\n",
        "```python\n",
        "grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "  (_, logits), grads = grad_fn(state.params)\n",
        "  grads = jax.lax.pmean(grads, axis_name=\"batch\")\n",
        "  new_state = state.apply_gradients(grads=grads)\n",
        "  metrics = compute_metrics(logits, targets, weights)\n",
        "  metrics[\"learning_rate\"] = learning_rate_fn(step)\n",
        "```\n",
        "\n",
        "jax.value_and_grad에 오차함수를 has_aux인자와 함께 넣어서 새로운 grad_fn을 만든다.\n",
        "\n",
        "이후에 우리의 `state`에 `.params`를 `grad_fn`에 넣고 grad변수를 `apply_gradient`로 적용시킨다.\n",
        "\n",
        "`jax.lax.pmean`은 8개로 분산시킨 배열의 gradient의 평균으로 정규화한다는 것이다.\n",
        "\n",
        "`grad`변수는 frozen_dict으로 되어있으며, 이것을 가지고 `wandb.log`에 집어넣는 것이 가능하리라 생각이 된다."
      ],
      "metadata": {
        "id": "TgnKiiz-MC7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flatten_grad = frozen_dict_gradient_flatten(grads)"
      ],
      "metadata": {
        "id": "El6IlcuNETOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flatten_grad = frozen_dict_gradient_flatten(grads)\n",
        "wandb_histo = {layers: wandb.Histogram(grads) for layers, grads in flatten_grad.items()}\n",
        "print(wandb_histo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sM87_2VdWuBy",
        "outputId": "b2ebe957-9859-468c-a0d3-28a58b4ad2e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Main/params/decoder/encoderdecoder_norm/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d2510>, 'Main/params/decoder/encoderdecoder_norm/scale': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986aa8d0>, 'Main/params/decoder/encoderdecoderblock_0/LayerNorm_0/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a99586f10>, 'Main/params/decoder/encoderdecoderblock_0/LayerNorm_0/scale': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9a86ea10>, 'Main/params/decoder/encoderdecoderblock_0/LayerNorm_1/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9a86ee50>, 'Main/params/decoder/encoderdecoderblock_0/LayerNorm_1/scale': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a99b19ad0>, 'Main/params/decoder/encoderdecoderblock_0/LayerNorm_2/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a99b192d0>, 'Main/params/decoder/encoderdecoderblock_0/LayerNorm_2/scale': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a99b19a10>, 'Main/params/decoder/encoderdecoderblock_0/MlpBlock_0/Dense_0/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d2f50>, 'Main/params/decoder/encoderdecoderblock_0/MlpBlock_0/Dense_0/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d22d0>, 'Main/params/decoder/encoderdecoderblock_0/MlpBlock_0/Dense_1/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d20d0>, 'Main/params/decoder/encoderdecoderblock_0/MlpBlock_0/Dense_1/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d2a10>, 'Main/params/decoder/encoderdecoderblock_0/MultiHeadDotProductAttention_0/key/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d2050>, 'Main/params/decoder/encoderdecoderblock_0/MultiHeadDotProductAttention_0/out/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d2390>, 'Main/params/decoder/encoderdecoderblock_0/MultiHeadDotProductAttention_0/query/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d2e10>, 'Main/params/decoder/encoderdecoderblock_0/MultiHeadDotProductAttention_0/value/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d2b50>, 'Main/params/decoder/encoderdecoderblock_0/SelfAttention_0/key/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d2850>, 'Main/params/decoder/encoderdecoderblock_0/SelfAttention_0/out/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d2b10>, 'Main/params/decoder/encoderdecoderblock_0/SelfAttention_0/query/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d2d50>, 'Main/params/decoder/encoderdecoderblock_0/SelfAttention_0/value/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d2f10>, 'Main/params/decoder/encoderdecoderblock_1/LayerNorm_0/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a994d0d90>, 'Main/params/decoder/encoderdecoderblock_1/LayerNorm_0/scale': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a99e2af50>, 'Main/params/decoder/encoderdecoderblock_1/LayerNorm_1/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a98eff090>, 'Main/params/decoder/encoderdecoderblock_1/LayerNorm_1/scale': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986bf250>, 'Main/params/decoder/encoderdecoderblock_1/LayerNorm_2/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986bf4d0>, 'Main/params/decoder/encoderdecoderblock_1/LayerNorm_2/scale': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986bf290>, 'Main/params/decoder/encoderdecoderblock_1/MlpBlock_0/Dense_0/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986bf710>, 'Main/params/decoder/encoderdecoderblock_1/MlpBlock_0/Dense_0/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986bf410>, 'Main/params/decoder/encoderdecoderblock_1/MlpBlock_0/Dense_1/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986bf850>, 'Main/params/decoder/encoderdecoderblock_1/MlpBlock_0/Dense_1/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986bf210>, 'Main/params/decoder/encoderdecoderblock_1/MultiHeadDotProductAttention_0/key/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986bf0d0>, 'Main/params/decoder/encoderdecoderblock_1/MultiHeadDotProductAttention_0/out/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986bffd0>, 'Main/params/decoder/encoderdecoderblock_1/MultiHeadDotProductAttention_0/query/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986bf990>, 'Main/params/decoder/encoderdecoderblock_1/MultiHeadDotProductAttention_0/value/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986bfd10>, 'Main/params/decoder/encoderdecoderblock_1/SelfAttention_0/key/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986bfc50>, 'Main/params/decoder/encoderdecoderblock_1/SelfAttention_0/out/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986bf490>, 'Main/params/decoder/encoderdecoderblock_1/SelfAttention_0/query/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986bfdd0>, 'Main/params/decoder/encoderdecoderblock_1/SelfAttention_0/value/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986bfe50>, 'Main/params/decoder/encoderdecoderblock_2/LayerNorm_0/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986bf810>, 'Main/params/decoder/encoderdecoderblock_2/LayerNorm_0/scale': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986bf3d0>, 'Main/params/decoder/encoderdecoderblock_2/LayerNorm_1/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986bf510>, 'Main/params/decoder/encoderdecoderblock_2/LayerNorm_1/scale': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986bf5d0>, 'Main/params/decoder/encoderdecoderblock_2/LayerNorm_2/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986bf350>, 'Main/params/decoder/encoderdecoderblock_2/LayerNorm_2/scale': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986bf950>, 'Main/params/decoder/encoderdecoderblock_2/MlpBlock_0/Dense_0/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986bf390>, 'Main/params/decoder/encoderdecoderblock_2/MlpBlock_0/Dense_0/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986bfb10>, 'Main/params/decoder/encoderdecoderblock_2/MlpBlock_0/Dense_1/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986bf190>, 'Main/params/decoder/encoderdecoderblock_2/MlpBlock_0/Dense_1/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986bfb90>, 'Main/params/decoder/encoderdecoderblock_2/MultiHeadDotProductAttention_0/key/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986bf090>, 'Main/params/decoder/encoderdecoderblock_2/MultiHeadDotProductAttention_0/out/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986bfbd0>, 'Main/params/decoder/encoderdecoderblock_2/MultiHeadDotProductAttention_0/query/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986bf150>, 'Main/params/decoder/encoderdecoderblock_2/MultiHeadDotProductAttention_0/value/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986bf610>, 'Main/params/decoder/encoderdecoderblock_2/SelfAttention_0/key/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986bf650>, 'Main/params/decoder/encoderdecoderblock_2/SelfAttention_0/out/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986bfed0>, 'Main/params/decoder/encoderdecoderblock_2/SelfAttention_0/query/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986bfc90>, 'Main/params/decoder/encoderdecoderblock_2/SelfAttention_0/value/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986bfb50>, 'Main/params/decoder/encoderdecoderblock_3/LayerNorm_0/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986bff90>, 'Main/params/decoder/encoderdecoderblock_3/LayerNorm_0/scale': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921ee10>, 'Main/params/decoder/encoderdecoderblock_3/LayerNorm_1/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921e810>, 'Main/params/decoder/encoderdecoderblock_3/LayerNorm_1/scale': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921ed90>, 'Main/params/decoder/encoderdecoderblock_3/LayerNorm_2/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921e7d0>, 'Main/params/decoder/encoderdecoderblock_3/LayerNorm_2/scale': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921ec50>, 'Main/params/decoder/encoderdecoderblock_3/MlpBlock_0/Dense_0/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921e310>, 'Main/params/decoder/encoderdecoderblock_3/MlpBlock_0/Dense_0/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921e450>, 'Main/params/decoder/encoderdecoderblock_3/MlpBlock_0/Dense_1/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921e710>, 'Main/params/decoder/encoderdecoderblock_3/MlpBlock_0/Dense_1/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921eb10>, 'Main/params/decoder/encoderdecoderblock_3/MultiHeadDotProductAttention_0/key/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921e3d0>, 'Main/params/decoder/encoderdecoderblock_3/MultiHeadDotProductAttention_0/out/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921ead0>, 'Main/params/decoder/encoderdecoderblock_3/MultiHeadDotProductAttention_0/query/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921eb50>, 'Main/params/decoder/encoderdecoderblock_3/MultiHeadDotProductAttention_0/value/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921e210>, 'Main/params/decoder/encoderdecoderblock_3/SelfAttention_0/key/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921e250>, 'Main/params/decoder/encoderdecoderblock_3/SelfAttention_0/out/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921e890>, 'Main/params/decoder/encoderdecoderblock_3/SelfAttention_0/query/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921e950>, 'Main/params/decoder/encoderdecoderblock_3/SelfAttention_0/value/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921ea50>, 'Main/params/decoder/encoderdecoderblock_4/LayerNorm_0/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921e5d0>, 'Main/params/decoder/encoderdecoderblock_4/LayerNorm_0/scale': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921e690>, 'Main/params/decoder/encoderdecoderblock_4/LayerNorm_1/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921e2d0>, 'Main/params/decoder/encoderdecoderblock_4/LayerNorm_1/scale': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921ee50>, 'Main/params/decoder/encoderdecoderblock_4/LayerNorm_2/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921ed50>, 'Main/params/decoder/encoderdecoderblock_4/LayerNorm_2/scale': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921ea90>, 'Main/params/decoder/encoderdecoderblock_4/MlpBlock_0/Dense_0/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921ea10>, 'Main/params/decoder/encoderdecoderblock_4/MlpBlock_0/Dense_0/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921e8d0>, 'Main/params/decoder/encoderdecoderblock_4/MlpBlock_0/Dense_1/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921e850>, 'Main/params/decoder/encoderdecoderblock_4/MlpBlock_0/Dense_1/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921e790>, 'Main/params/decoder/encoderdecoderblock_4/MultiHeadDotProductAttention_0/key/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921e650>, 'Main/params/decoder/encoderdecoderblock_4/MultiHeadDotProductAttention_0/out/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921e750>, 'Main/params/decoder/encoderdecoderblock_4/MultiHeadDotProductAttention_0/query/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921ef90>, 'Main/params/decoder/encoderdecoderblock_4/MultiHeadDotProductAttention_0/value/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921e1d0>, 'Main/params/decoder/encoderdecoderblock_4/SelfAttention_0/key/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921ec10>, 'Main/params/decoder/encoderdecoderblock_4/SelfAttention_0/out/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921e150>, 'Main/params/decoder/encoderdecoderblock_4/SelfAttention_0/query/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921e590>, 'Main/params/decoder/encoderdecoderblock_4/SelfAttention_0/value/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921e910>, 'Main/params/decoder/encoderdecoderblock_5/LayerNorm_0/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9921ef50>, 'Main/params/decoder/encoderdecoderblock_5/LayerNorm_0/scale': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a98a3ee10>, 'Main/params/decoder/encoderdecoderblock_5/LayerNorm_1/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a98c131d0>, 'Main/params/decoder/encoderdecoderblock_5/LayerNorm_1/scale': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a98c13290>, 'Main/params/decoder/encoderdecoderblock_5/LayerNorm_2/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a98c13ad0>, 'Main/params/decoder/encoderdecoderblock_5/LayerNorm_2/scale': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a98c13790>, 'Main/params/decoder/encoderdecoderblock_5/MlpBlock_0/Dense_0/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a98c13c90>, 'Main/params/decoder/encoderdecoderblock_5/MlpBlock_0/Dense_0/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a98c13810>, 'Main/params/decoder/encoderdecoderblock_5/MlpBlock_0/Dense_1/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a99fc4450>, 'Main/params/decoder/encoderdecoderblock_5/MlpBlock_0/Dense_1/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5d90>, 'Main/params/decoder/encoderdecoderblock_5/MultiHeadDotProductAttention_0/key/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d50d0>, 'Main/params/decoder/encoderdecoderblock_5/MultiHeadDotProductAttention_0/out/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5f50>, 'Main/params/decoder/encoderdecoderblock_5/MultiHeadDotProductAttention_0/query/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5990>, 'Main/params/decoder/encoderdecoderblock_5/MultiHeadDotProductAttention_0/value/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5790>, 'Main/params/decoder/encoderdecoderblock_5/SelfAttention_0/key/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d52d0>, 'Main/params/decoder/encoderdecoderblock_5/SelfAttention_0/out/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5e50>, 'Main/params/decoder/encoderdecoderblock_5/SelfAttention_0/query/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5bd0>, 'Main/params/decoder/encoderdecoderblock_5/SelfAttention_0/value/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5290>, 'Main/params/decoder/logitdense/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5410>, 'Main/params/decoder/logitdense/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5110>, 'Main/params/encoder/encoder_norm/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d51d0>, 'Main/params/encoder/encoder_norm/scale': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5510>, 'Main/params/encoder/encoderblock_0/LayerNorm_0/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d53d0>, 'Main/params/encoder/encoderblock_0/LayerNorm_0/scale': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5cd0>, 'Main/params/encoder/encoderblock_0/LayerNorm_1/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5050>, 'Main/params/encoder/encoderblock_0/LayerNorm_1/scale': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5d10>, 'Main/params/encoder/encoderblock_0/MlpBlock_0/Dense_0/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d55d0>, 'Main/params/encoder/encoderblock_0/MlpBlock_0/Dense_0/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5450>, 'Main/params/encoder/encoderblock_0/MlpBlock_0/Dense_1/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5f10>, 'Main/params/encoder/encoderblock_0/MlpBlock_0/Dense_1/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5dd0>, 'Main/params/encoder/encoderblock_0/SelfAttention_0/key/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d58d0>, 'Main/params/encoder/encoderblock_0/SelfAttention_0/out/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5690>, 'Main/params/encoder/encoderblock_0/SelfAttention_0/query/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5810>, 'Main/params/encoder/encoderblock_0/SelfAttention_0/value/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d54d0>, 'Main/params/encoder/encoderblock_1/LayerNorm_0/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5350>, 'Main/params/encoder/encoderblock_1/LayerNorm_0/scale': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5490>, 'Main/params/encoder/encoderblock_1/LayerNorm_1/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5890>, 'Main/params/encoder/encoderblock_1/LayerNorm_1/scale': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5390>, 'Main/params/encoder/encoderblock_1/MlpBlock_0/Dense_0/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d59d0>, 'Main/params/encoder/encoderblock_1/MlpBlock_0/Dense_0/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5590>, 'Main/params/encoder/encoderblock_1/MlpBlock_0/Dense_1/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5090>, 'Main/params/encoder/encoderblock_1/MlpBlock_0/Dense_1/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5850>, 'Main/params/encoder/encoderblock_1/SelfAttention_0/key/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d56d0>, 'Main/params/encoder/encoderblock_1/SelfAttention_0/out/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5b90>, 'Main/params/encoder/encoderblock_1/SelfAttention_0/query/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5c90>, 'Main/params/encoder/encoderblock_1/SelfAttention_0/value/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5550>, 'Main/params/encoder/encoderblock_2/LayerNorm_0/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5e10>, 'Main/params/encoder/encoderblock_2/LayerNorm_0/scale': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5b50>, 'Main/params/encoder/encoderblock_2/LayerNorm_1/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5fd0>, 'Main/params/encoder/encoderblock_2/LayerNorm_1/scale': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5210>, 'Main/params/encoder/encoderblock_2/MlpBlock_0/Dense_0/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5190>, 'Main/params/encoder/encoderblock_2/MlpBlock_0/Dense_0/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5c50>, 'Main/params/encoder/encoderblock_2/MlpBlock_0/Dense_1/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d5d50>, 'Main/params/encoder/encoderblock_2/MlpBlock_0/Dense_1/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a879b6050>, 'Main/params/encoder/encoderblock_2/SelfAttention_0/key/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a879b6090>, 'Main/params/encoder/encoderblock_2/SelfAttention_0/out/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a879b60d0>, 'Main/params/encoder/encoderblock_2/SelfAttention_0/query/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a879b6110>, 'Main/params/encoder/encoderblock_2/SelfAttention_0/value/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a879b6150>, 'Main/params/encoder/encoderblock_3/LayerNorm_0/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a879b6190>, 'Main/params/encoder/encoderblock_3/LayerNorm_0/scale': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a879b61d0>, 'Main/params/encoder/encoderblock_3/LayerNorm_1/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a879b6210>, 'Main/params/encoder/encoderblock_3/LayerNorm_1/scale': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a879b6250>, 'Main/params/encoder/encoderblock_3/MlpBlock_0/Dense_0/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d2910>, 'Main/params/encoder/encoderblock_3/MlpBlock_0/Dense_0/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d2250>, 'Main/params/encoder/encoderblock_3/MlpBlock_0/Dense_1/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a98bcc490>, 'Main/params/encoder/encoderblock_3/MlpBlock_0/Dense_1/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9a86e250>, 'Main/params/encoder/encoderblock_3/SelfAttention_0/key/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9a86e310>, 'Main/params/encoder/encoderblock_3/SelfAttention_0/out/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9a86ee90>, 'Main/params/encoder/encoderblock_3/SelfAttention_0/query/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9a86ec50>, 'Main/params/encoder/encoderblock_3/SelfAttention_0/value/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9a86e510>, 'Main/params/encoder/encoderblock_4/LayerNorm_0/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9a86e050>, 'Main/params/encoder/encoderblock_4/LayerNorm_0/scale': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9a86e750>, 'Main/params/encoder/encoderblock_4/LayerNorm_1/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9a86e7d0>, 'Main/params/encoder/encoderblock_4/LayerNorm_1/scale': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9a86e410>, 'Main/params/encoder/encoderblock_4/MlpBlock_0/Dense_0/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9a86ec90>, 'Main/params/encoder/encoderblock_4/MlpBlock_0/Dense_0/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9a86e690>, 'Main/params/encoder/encoderblock_4/MlpBlock_0/Dense_1/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9a86e090>, 'Main/params/encoder/encoderblock_4/MlpBlock_0/Dense_1/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9a86ebd0>, 'Main/params/encoder/encoderblock_4/SelfAttention_0/key/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9a86e850>, 'Main/params/encoder/encoderblock_4/SelfAttention_0/out/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9a86e650>, 'Main/params/encoder/encoderblock_4/SelfAttention_0/query/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9a86e150>, 'Main/params/encoder/encoderblock_4/SelfAttention_0/value/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9a86ea90>, 'Main/params/encoder/encoderblock_5/LayerNorm_0/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9a86ecd0>, 'Main/params/encoder/encoderblock_5/LayerNorm_0/scale': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9a86e590>, 'Main/params/encoder/encoderblock_5/LayerNorm_1/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9a86eed0>, 'Main/params/encoder/encoderblock_5/LayerNorm_1/scale': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9a86e710>, 'Main/params/encoder/encoderblock_5/MlpBlock_0/Dense_0/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9a86e210>, 'Main/params/encoder/encoderblock_5/MlpBlock_0/Dense_0/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9a86ef50>, 'Main/params/encoder/encoderblock_5/MlpBlock_0/Dense_1/bias': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9a86eb10>, 'Main/params/encoder/encoderblock_5/MlpBlock_0/Dense_1/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9a86e1d0>, 'Main/params/encoder/encoderblock_5/SelfAttention_0/key/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9a86e0d0>, 'Main/params/encoder/encoderblock_5/SelfAttention_0/out/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9a86e350>, 'Main/params/encoder/encoderblock_5/SelfAttention_0/query/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9a86e4d0>, 'Main/params/encoder/encoderblock_5/SelfAttention_0/value/kernel': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a9a86e910>, 'Main/params/shared_embedding/embedding': <wandb.sdk.data_types.histogram.Histogram object at 0x7f4a986d7ed0>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "6A8J3aoZAeZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "s_6KWlm1AkaS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}